{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599277314066",
   "display_name": "Python 3.7.6 64-bit ('miniconda3': virtualenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "DirichletMultinomial = tfp.distributions.DirichletMultinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def session_options(enable_gpu_ram_resizing=True):\n",
    "  \"\"\"Convenience function which sets common `tf.Session` options.\"\"\"\n",
    "  config = tf.ConfigProto()\n",
    "  config.log_device_placement = True\n",
    "  if enable_gpu_ram_resizing:\n",
    "    # `allow_growth=True` makes it possible to connect multiple colabs to your\n",
    "    # GPU. Otherwise the colab malloc's all GPU ram.\n",
    "    config.gpu_options.allow_growth = True\n",
    "  return config\n",
    "\n",
    "def reset_sess(config=None):\n",
    "  \"\"\"Convenience function to create the TF graph and session, or reset them.\"\"\"\n",
    "  if config is None:\n",
    "    config = session_options()\n",
    "  tf.reset_default_graph()\n",
    "  global sess\n",
    "  try:\n",
    "    sess.close()\n",
    "  except:\n",
    "    pass\n",
    "  sess = tf.InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "rng = np.random.RandomState(seed=45)\n",
    "tf.random.set_seed(76)\n",
    "\n",
    "# Precision\n",
    "dtype = np.float64\n",
    "\n",
    "# Number of training samples\n",
    "num_samples = 50000\n",
    "\n",
    "# Ground truth loc values which we will infer later on. The scale is 1.\n",
    "true_loc = np.array([[-4, -4],\n",
    "                     [0, 0],\n",
    "                     [4, 4]], dtype)\n",
    "\n",
    "true_components_num, dims = true_loc.shape\n",
    "\n",
    "# Generate training samples from ground truth loc\n",
    "true_hidden_component = rng.randint(0, true_components_num, num_samples)\n",
    "observations = (true_loc[true_hidden_component]\n",
    "\n",
    "                + rng.randn(num_samples, dims).astype(dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [1., 2., 3.]\n",
    "n = 2.\n",
    "dist = DirichletMultinomial(total_count=n, concentration=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tfp.distributions.DirichletMultinomial 'DirichletMultinomial' batch_shape=[] event_shape=[3] dtype=float32>"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.00018037511>"
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "d = dist.prob(tf.constant([1.,2,3]))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)>"
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "tf.constant([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'T'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-705c012d26ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "d.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-61-754b79bd55c9>, line 75)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-61-754b79bd55c9>\"\u001b[0;36m, line \u001b[0;32m75\u001b[0m\n\u001b[0;31m    trajectoryLLs.append(ll)\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "def tfNullLikelihood(pDsAll, altCounts):\n",
    "    print(\"pDS are\", pDsAll)\n",
    "    print(\"altCounts are\", altCounts)\n",
    "    return tfd.Multinomial(probs=pDsAll).prob(altCounts)\n",
    "\n",
    "def tfEffectLikelihood(nHypotheses, pDs, altCountsFlat):\n",
    "    nGenes = altCountsFlat.shape[0]\n",
    "    nConditions = altCountsFlat.shape[1]\n",
    "    nHypothesesNonNull = nHypotheses - 1\n",
    "\n",
    "    # nGenes x 1\n",
    "    n = tf.reduce_sum(altCountsFlat, 1) #xCtrl + xCase1 + xCase2 + xCase12\n",
    "    print(\"n\", n)\n",
    "    altCountsShaped = tf.broadcast_to(altCountsFlat, [\n",
    "        nHypothesesNonNull, nGenes, nConditions])\n",
    "    altCountsShaped = tf.transpose(altCountsShaped, [0, 1])\n",
    "\n",
    "    altCountsShaped = tf.broadcast_to(altCountsFlat, [nHypothesesNonNull, nGenes, nConditions])\n",
    "    altCountsShaped = tf.transpose(altCountsShaped, [0, 1])\n",
    "\n",
    "    nShaped = tf.transpose(tf.broadcast_to(n, [nHypothesesNonNull, nGenes]))\n",
    "\n",
    "    pDsum = tf.reduce_sum(pDs)\n",
    "    pdsAll = tf.constant([1 - pDsum, *pDs])\n",
    "    pdsAllShaped = tf.broadcast_to(pdsAll, [nHypothesesNonNull, nConditions])\n",
    "\n",
    "    def likelihoodFn(alpha0, alpha1, alpha2, alphaBoth):\n",
    "        concentrations = tf.constant([\n",
    "            [alpha0, alpha1, alpha0, alpha1],  # H1\n",
    "            [alpha0, alpha0, alpha2, alpha2],  # H2\n",
    "            [alpha0, alpha1 + alphaBoth, alpha2 + alphaBoth,\n",
    "                alpha1 + alpha2 + alphaBoth]  # H1&2&3\n",
    "        ])\n",
    "\n",
    "        concentrations = tf.broadcast_to(concentrations, [nGenes, nHypothesesNonNull, nConditions])\n",
    "        concentrations = pdsAllShaped * concentrations\n",
    "\n",
    "        # try to stay in the log space; avoid numeric underflow, until bayes factor calc\n",
    "        return tfd.DirichletMultinomial(total_count=nShaped, concentration=concentrations).prob(altCountsShaped)\n",
    "\n",
    "    return likelihoodFn, nullLikelihood(pDs, altCountsFlat)\n",
    "\n",
    "def tfLikelihoodBivariateDM(altCountsFlat: tf.Tensor, pDs: tf.Tensor, trajectoryPis, trajectoryAlphas, trajectoryLLs):\n",
    "    print(altCountsFlat.shape)\n",
    "\n",
    "    # TODO: make this flexible for multivariate\n",
    "    nHypotheses = 4\n",
    "    nGenes = altCountsFlat.shape[0]\n",
    "\n",
    "    likelihoodFn, allNull2, = effectLikelihood(nHypotheses, pDs, altCountsFlat)\n",
    "\n",
    "    def jointLikelihood(params):\n",
    "        pi1, pi2, piBoth, alpha0, alpha1, alpha2, alphaBoth = params\n",
    "\n",
    "        if alpha0 < 0 or alpha1 < 0 or alpha2 < 0 or alphaBoth < 0 or pi1 < 0 or pi2 < 0 or piBoth < 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        pi0 = 1.0 - (pi1 + pi2 + piBoth)\n",
    "\n",
    "        if pi0 < 0:\n",
    "            return float(\"inf\")\n",
    "\n",
    "        h0 = pi0 * allNull2\n",
    "\n",
    "        trajectoryPis.append([pi1, pi2, piBoth])\n",
    "        trajectoryAlphas.append([alpha0, alpha1, alpha2, alphaBoth])\n",
    "\n",
    "        hs = tf.constant([[pi1, pi2, piBoth]]) * likelihoodFn(alpha0, alpha1, alpha2, alphaBoth)\n",
    "\n",
    "        ll = tf.reduce_sum(-tf.math.log(h0 + tf.reduce_sum(hs, 1))\n",
    "        trajectoryLLs.append(ll)\n",
    "        return ll\n",
    "\n",
    "    return jointLikelihood\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirichletMultinomial = tfp.distributions.DirichletMultinomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mDirichletMultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtotal_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mconcentration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'DirichletMultinomial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nDirichlet-Multinomial compound distribution.\n\nThe Dirichlet-Multinomial distribution is parameterized by a (batch of)\nlength-`K` `concentration` vectors (`K > 1`) and a `total_count` number of\ntrials, i.e., the number of trials per draw from the DirichletMultinomial. It\nis defined over a (batch of) length-`K` vector `counts` such that\n`tf.reduce_sum(counts, -1) = total_count`. The Dirichlet-Multinomial is\nidentically the Beta-Binomial distribution when `K = 2`.\n\n#### Mathematical Details\n\nThe Dirichlet-Multinomial is a distribution over `K`-class counts, i.e., a\nlength-`K` vector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\nThe probability mass function (pmf) is,\n\n```none\npmf(n; alpha, N) = Beta(alpha + n) / (prod_j n_j!) / Z\nZ = Beta(alpha) / N!\n```\n\nwhere:\n\n* `concentration = alpha = [alpha_0, ..., alpha_{K-1}]`, `alpha_j > 0`,\n* `total_count = N`, `N` a positive integer,\n* `N!` is `N` factorial, and,\n* `Beta(x) = prod_j Gamma(x_j) / Gamma(sum_j x_j)` is the\n  [multivariate beta function](\n  https://en.wikipedia.org/wiki/Beta_function#Multivariate_beta_function),\n  and,\n* `Gamma` is the [gamma function](\n  https://en.wikipedia.org/wiki/Gamma_function).\n\nDirichlet-Multinomial is a [compound distribution](\nhttps://en.wikipedia.org/wiki/Compound_probability_distribution), i.e., its\nsamples are generated as follows.\n\n  1. Choose class probabilities:\n     `probs = [p_0,...,p_{K-1}] ~ Dir(concentration)`\n  2. Draw integers:\n     `counts = [n_0,...,n_{K-1}] ~ Multinomial(total_count, probs)`\n\nThe last `concentration` dimension parameterizes a single\nDirichlet-Multinomial distribution. When calling distribution functions\n(e.g., `dist.prob(counts)`), `concentration`, `total_count` and `counts` are\nbroadcast to the same shape. The last dimension of `counts` corresponds to\nsingle Dirichlet-Multinomial distributions.\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\n#### Pitfalls\n\nThe number of classes, `K`, must not exceed:\n\n- the largest integer representable by `self.dtype`, i.e.,\n  `2**(mantissa_bits+1)` (IEE754),\n- the maximum `Tensor` index, i.e., `2**31-1`.\n\nIn other words,\n\n```python\nK <= min(2**31-1, {\n  tf.float16: 2**11,\n  tf.float32: 2**24,\n  tf.float64: 2**53 }[param.dtype])\n```\n\nNote: This condition is validated only when `self.validate_args = True`.\n\n#### Examples\n\n```python\nalpha = [1., 2., 3.]\nn = 2.\ndist = DirichletMultinomial(n, alpha)\n```\n\nCreates a 3-class distribution, with the 3rd class is most likely to be\ndrawn.\nThe distribution functions can be evaluated on counts.\n\n```python\n# counts same shape as alpha.\ncounts = [0., 0., 2.]\ndist.prob(counts)  # Shape []\n\n# alpha will be broadcast to [[1., 2., 3.], [1., 2., 3.]] to match counts.\ncounts = [[1., 1., 0.], [1., 0., 1.]]\ndist.prob(counts)  # Shape [2]\n\n# alpha will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7]\n```\n\nCreates a 2-batch of 3-class distributions.\n\n```python\nalpha = [[1., 2., 3.], [4., 5., 6.]]  # Shape [2, 3]\nn = [3., 3.]\ndist = DirichletMultinomial(n, alpha)\n\n# counts will be broadcast to [[2., 1., 0.], [2., 1., 0.]] to match alpha.\ncounts = [2., 1., 0.]\ndist.prob(counts)  # Shape [2]\n```\n\u001b[0;31mInit docstring:\u001b[0m\nInitialize a batch of DirichletMultinomial distributions.\n\nArgs:\n  total_count: Non-negative integer-valued tensor, whose dtype is the same\n    as `concentration`. The shape is broadcastable to `[N1,..., Nm]` with\n    `m >= 0`. Defines this as a batch of `N1 x ... x Nm` different\n    Dirichlet multinomial distributions. Its components should be equal to\n    integer values.\n  concentration: Positive floating point tensor with shape broadcastable to\n    `[N1,..., Nm, K]` `m >= 0`.  Defines this as a batch of `N1 x ... x Nm`\n    different `K` class Dirichlet multinomial distributions.\n  validate_args: Python `bool`, default `False`. When `True` distribution\n    parameters are checked for validity despite possibly degrading runtime\n    performance. When `False` invalid inputs may silently render incorrect\n    outputs.\n allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n    (e.g., mean, variance) use the value \"`NaN`\" to indicate the result is\n    undefined. When `False`, an exception is raised if one or more of the\n    statistic's batch members are undefined.\n name: Python `str` name prefixed to Ops created by this class.\n\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/dirichlet_multinomial.py\n\u001b[0;31mType:\u001b[0m           _DistributionMeta\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "DirichletMultinomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mtfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtotal_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mvalidate_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mallow_nan_stats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Multinomial'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nMultinomial distribution.\n\nThis Multinomial distribution is parameterized by `probs`, a (batch of)\nlength-`K` `prob` (probability) vectors (`K > 1`) such that\n`tf.reduce_sum(probs, -1) = 1`, and a `total_count` number of trials, i.e.,\nthe number of trials per draw from the Multinomial. It is defined over a\n(batch of) length-`K` vector `counts` such that\n`tf.reduce_sum(counts, -1) = total_count`. The Multinomial is identically the\nBinomial distribution when `K = 2`.\n\n#### Mathematical Details\n\nThe Multinomial is a distribution over `K`-class counts, i.e., a length-`K`\nvector of non-negative integer `counts = n = [n_0, ..., n_{K-1}]`.\n\nThe probability mass function (pmf) is,\n\n```none\npmf(n; pi, N) = prod_j (pi_j)**n_j / Z\nZ = (prod_j n_j!) / N!\n```\n\nwhere:\n* `probs = pi = [pi_0, ..., pi_{K-1}]`, `pi_j > 0`, `sum_j pi_j = 1`,\n* `total_count = N`, `N` a positive integer,\n* `Z` is the normalization constant, and,\n* `N!` denotes `N` factorial.\n\nDistribution parameters are automatically broadcast in all functions; see\nexamples for details.\n\n#### Pitfalls\n\nThe number of classes, `K`, must not exceed:\n\n- the largest integer representable by `self.dtype`, i.e.,\n  `2**(mantissa_bits+1)` (IEE754),\n- the maximum `Tensor` index, i.e., `2**31-1`.\n\nIn other words,\n\n```python\nK <= min(2**31-1, {\n  tf.float16: 2**11,\n  tf.float32: 2**24,\n  tf.float64: 2**53 }[param.dtype])\n```\n\nNote: This condition is validated only when `self.validate_args = True`.\n\n#### Examples\n\nCreate a 3-class distribution, with the 3rd class is most likely to be drawn,\nusing logits.\n\n```python\nlogits = [-50., -43, 0]\ndist = Multinomial(total_count=4., logits=logits)\n```\n\nCreate a 3-class distribution, with the 3rd class is most likely to be drawn.\n\n```python\np = [.2, .3, .5]\ndist = Multinomial(total_count=4., probs=p)\n```\n\nThe distribution functions can be evaluated on counts.\n\n```python\n# counts same shape as p.\ncounts = [1., 0, 3]\ndist.prob(counts)  # Shape []\n\n# p will be broadcast to [[.2, .3, .5], [.2, .3, .5]] to match counts.\ncounts = [[1., 2, 1], [2, 2, 0]]\ndist.prob(counts)  # Shape [2]\n\n# p will be broadcast to shape [5, 7, 3] to match counts.\ncounts = [[...]]  # Shape [5, 7, 3]\ndist.prob(counts)  # Shape [5, 7]\n```\n\nCreate a 2-batch of 3-class distributions.\n\n```python\np = [[.1, .2, .7], [.3, .3, .4]]  # Shape [2, 3]\ndist = Multinomial(total_count=[4., 5], probs=p)\n\ncounts = [[2., 1, 1], [3, 1, 1]]\ndist.prob(counts)  # Shape [2]\n\ndist.sample(5) # Shape [5, 2, 3]\n```\n\u001b[0;31mInit docstring:\u001b[0m\nInitialize a batch of Multinomial distributions.\n\nArgs:\n  total_count: Non-negative floating point tensor with shape broadcastable\n    to `[N1,..., Nm]` with `m >= 0`. Defines this as a batch of\n    `N1 x ... x Nm` different Multinomial distributions. Its components\n    should be equal to integer values.\n  logits: Floating point tensor representing unnormalized log-probabilities\n    of a positive event with shape broadcastable to\n    `[N1,..., Nm, K]` `m >= 0`, and the same dtype as `total_count`. Defines\n    this as a batch of `N1 x ... x Nm` different `K` class Multinomial\n    distributions. Only one of `logits` or `probs` should be passed in.\n  probs: Positive floating point tensor with shape broadcastable to\n    `[N1,..., Nm, K]` `m >= 0` and same dtype as `total_count`. Defines\n    this as a batch of `N1 x ... x Nm` different `K` class Multinomial\n    distributions. `probs`'s components in the last portion of its shape\n    should sum to `1`. Only one of `logits` or `probs` should be passed in.\n  validate_args: Python `bool`, default `False`. When `True` distribution\n    parameters are checked for validity despite possibly degrading runtime\n    performance. When `False` invalid inputs may silently render incorrect\n    outputs.\n  allow_nan_stats: Python `bool`, default `True`. When `True`, statistics\n    (e.g., mean, mode, variance) use the value \"`NaN`\" to indicate the\n    result is undefined. When `False`, an exception is raised if one or\n    more of the statistic's batch members are undefined.\n  name: Python `str` name prefixed to Ops created by this class.\n\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.7/site-packages/tensorflow_probability/python/distributions/multinomial.py\n\u001b[0;31mType:\u001b[0m           _DistributionMeta\n\u001b[0;31mSubclasses:\u001b[0m     \n"
    }
   ],
   "source": [
    "tfd.Multinomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability\n",
    "import numpy as np\n",
    "tfd = tensorflow_probability.distributions\n",
    "reset_sess()\n",
    "\n",
    "# Upperbound on K\n",
    "max_cluster_num = 4\n",
    "\n",
    "# Define trainable variables.\n",
    "mix_probs = tf.nn.softmax(\n",
    "    tf.Variable(\n",
    "        name='mix_probs',\n",
    "        initial_value=np.ones([max_cluster_num], dtype) / max_cluster_num))\n",
    "\n",
    "concentrations = tf.Variable(\n",
    "    name='concentration',\n",
    "    initial_value=np.random.uniform(\n",
    "        low=1, #set around minimum value of sample value\n",
    "        high=25000, #set around maximum value of sample value\n",
    "        size=[max_cluster_num, dims]))\n",
    "\n",
    "precision = tf.nn.softplus(tf.Variable(\n",
    "    name='precision',\n",
    "    initial_value=\n",
    "    np.ones([max_cluster_num, dims], dtype=dtype)))\n",
    "\n",
    "alpha = tf.nn.softplus(tf.Variable(\n",
    "    name='alpha',\n",
    "    initial_value=\n",
    "    np.ones([1], dtype=dtype)))\n",
    "\n",
    "training_vals = [mix_probs, alpha, loc, precision]\n",
    "\n",
    "\n",
    "# Prior distributions of the training variables\n",
    "\n",
    "#Use symmetric Dirichlet prior as finite approximation of Dirichlet process.\n",
    "rv_symmetric_dirichlet_process = tfd.Dirichlet(\n",
    "    concentration=np.ones(max_cluster_num, dtype) * alpha / max_cluster_num,\n",
    "    name='rv_sdp')\n",
    "\n",
    "rv_loc = tfd.Independent(\n",
    "    tfd.Normal(\n",
    "        loc=tf.zeros([max_cluster_num, dims], dtype=dtype),\n",
    "        scale=tf.ones([max_cluster_num, dims], dtype=dtype)),\n",
    "    reinterpreted_batch_ndims=1,\n",
    "    name='rv_loc')\n",
    "\n",
    "\n",
    "rv_precision = tfd.Independent(\n",
    "    tfd.InverseGamma(\n",
    "        concentration=np.ones([max_cluster_num, dims], dtype),\n",
    "        rate=np.ones([max_cluster_num, dims], dtype)),\n",
    "    reinterpreted_batch_ndims=1,\n",
    "    name='rv_precision')\n",
    "\n",
    "rv_alpha = tfd.InverseGamma(\n",
    "    concentration=np.ones([1], dtype=dtype),\n",
    "    rate=np.ones([1]),\n",
    "    name='rv_alpha')\n",
    "\n",
    "# Define mixture model\n",
    "rv_observations = tfd.MixtureSameFamily(\n",
    "    mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
    "    components_distribution=tfd.MultivariateNormalDiag(\n",
    "        loc=loc,\n",
    "        scale_diag=precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = np.float64\n",
    "dims = 4\n",
    "components = 4\n",
    "num_samples = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgmm = tfd.JointDistributionNamed(dict(\n",
    "  mix_probs=tfd.Dirichlet(\n",
    "    concentration=np.ones(components, dtype) / 10.),\n",
    "  loc=tfd.Independent(\n",
    "    tfd.Normal(\n",
    "        loc=np.stack([\n",
    "            -np.ones(dims, dtype),\n",
    "            np.zeros(dims, dtype),\n",
    "            np.ones(dims, dtype),\n",
    "        ]),\n",
    "        scale=tf.ones([components, dims], dtype)),\n",
    "    reinterpreted_batch_ndims=2),\n",
    "  precision=tfd.Independent(\n",
    "    tfd.WishartTriL(\n",
    "        df=5,\n",
    "        scale_tril=np.stack([np.eye(dims, dtype=dtype)]*components),\n",
    "        input_output_cholesky=True),\n",
    "    reinterpreted_batch_ndims=1),\n",
    "  s=lambda mix_probs, loc, precision: tfd.Sample(tfd.MixtureSameFamily(\n",
    "      mixture_distribution=tfd.Categorical(probs=mix_probs),\n",
    "      components_distribution=MVNCholPrecisionTriL(\n",
    "          loc=loc,\n",
    "          chol_precision_tril=precision)),\n",
    "      sample_shape=num_samples)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
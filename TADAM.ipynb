{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: torch in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from torch) (1.18.2)\n",
      "Requirement already up-to-date: pyro-ppl in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (1.3.1)\n",
      "Requirement already satisfied, skipping upgrade: tqdm>=4.36 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyro-ppl) (4.42.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyro-ppl) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyro-ppl) (3.2.0)\n",
      "Requirement already satisfied, skipping upgrade: pyro-api>=0.1.1 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyro-ppl) (0.1.1)\n",
      "Requirement already satisfied, skipping upgrade: torch>=1.4.0 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyro-ppl) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: future in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from torch>=1.4.0->pyro-ppl) (0.18.2)\n",
      "Requirement already up-to-date: scipy in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scipy) (1.18.2)\n",
      "Requirement already up-to-date: matplotlib in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from matplotlib) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
      "Requirement already satisfied, skipping upgrade: six in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n",
      "Requirement already up-to-date: scikit-optimize in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (0.7.4)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scikit-optimize) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.0 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scikit-optimize) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: pyaml>=16.9 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scikit-optimize) (20.3.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.0 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scikit-optimize) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.19.1 in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from scikit-optimize) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML in /Users/alexkotlar/miniconda3/lib/python3.7/site-packages (from pyaml>=16.9->scikit-optimize) (5.3.1)\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "\n",
    "!pip install --upgrade torch\n",
    "!pip install --upgrade pyro-ppl\n",
    "!pip install --upgrade scipy\n",
    "!pip install --upgrade matplotlib\n",
    "!pip install --upgrade scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import torch\n",
    "import torch.tensor as tensor\n",
    "import pyro.distributions as dist\n",
    "# from torch.distributions import Binomial, Gamma, Uniform\n",
    "from pyro.distributions import Binomial, Bernoulli, Categorical, Dirichlet, DirichletMultinomial, Beta, BetaBinomial, Uniform, Gamma, Multinomial\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from skopt import gp_minimize \n",
    "from scipy.stats import binom as ScipyBinom\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from collections import namedtuple\n",
    "import time\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN\n"
     ]
    }
   ],
   "source": [
    "from mvl import genData, likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pDs are: tensor([0.0098, 0.0098, 0.0020])\n",
      "TESTING WITH: nCases tensor([5000., 5000., 1000.]) nCtrls tensor(500000.) rrMeans tensor([5., 5., 2.]) rrShape tensor(10.) afMean tensor(1.0000e-04) afShape tensor(10.) diseaseFractions tensor([0.0500, 0.0500, 0.0500]) pDs tensor([0.0098, 0.0098, 0.0020])\n",
      "rrDist mean tensor([4.9850, 4.9794, 1.9905])\n",
      "startIndices [0, tensor(1000.), tensor(2000.)] endIndices tensor([1000., 2000., 3000.])\n",
      "totalSamples 511000\n",
      "took 6.829218149185181\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "# genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "r5 = genData.v5(**genData.genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0])\n",
    "print(\"took\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r5afs = r5[\"afs\"]\n",
    "r5afs[0:1000].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "r6 = genData.v6(**genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0])\n",
    "print(\"took\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r6afs = r6[\"afs\"]\n",
    "r6afs[0:1000].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "r7 = genData7(**genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0])\n",
    "print(\"took\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "r7run2 = genData7(**genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0])\n",
    "print(\"took\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7afs = r7[\"afs\"]\n",
    "cBothgenes = r7[\"affectedGenes\"][2]\n",
    "\n",
    "diff57 = (abs(r7afs[cBothgenes] - r5afs[cBothgenes])/r7afs[cBothgenes])\n",
    "print(\"mean vs 5\", diff57.mean(0))\n",
    "\n",
    "r7run2afs = r7run2[\"afs\"]\n",
    "diffrun2 = (abs(r7afs[cBothgenes] - r7run2afs[cBothgenes])/r7afs[cBothgenes])\n",
    "print(\"mean vs 7run2\", diffrun2.mean(0))\n",
    "print(\"std vs 5\", diff57.std(0))\n",
    "print(\"std vs 7run2\", diffrun2.std(0))\n",
    "\n",
    "print(\"difference in means\",  diff57.mean(0) - diffrun2.mean(0))\n",
    "\n",
    "print(\"difference in std\",  diff57.std(0) - diffrun2.std(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7afs = r7[\"afs\"]\n",
    "cBothgenes = r7[\"affectedGenes\"][2]\n",
    "\n",
    "diff67 = (abs(r7afs[cBothgenes] - r6afs[cBothgenes])/r7afs[cBothgenes])\n",
    "print(\"mean vs 6\", diff67.mean(0))\n",
    "\n",
    "r7run2afs = r7run2[\"afs\"]\n",
    "diffrun2 = (abs(r7afs[cBothgenes] - r7run2afs[cBothgenes])/r7afs[cBothgenes])\n",
    "print(\"mean vs 7run2\", diffrun2.mean(0))\n",
    "print(\"std vs 5\", diff67.std(0))\n",
    "print(\"std vs 7run2\", diffrun2.std(0))\n",
    "\n",
    "print(\"difference in means\",  abs(diff67.mean(0) - diffrun2.mean(0)))\n",
    "\n",
    "print(\"difference in std\",  abs(diff67.std(0) - diffrun2.std(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r7[\"altCounts\"][3000:4000].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "# genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "rOld = genData4c(**genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0])\n",
    "print(\"took\", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rOld[0][3000:4000].mean(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(r[\"afs\"][2000:3000, 1], r[\"afs\"][2000:3000, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(flattenedData4)\n",
    "pyplot.clf()\n",
    "pyplot.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "pyplot.plot(cachedData4b[-1][0][\"afsPooled\"][:, 2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"corr for 1 & both in both\", np.corrcoef(afsFlatPooled4[4000:5000, 2], afsFlatPooled4[4000:5000, 3]))\n",
    "print(\"corr for 1 & both in 1only\", np.corrcoef(afsFlatPooled4[0:2000, 1], afsFlatPooled4[0:2000, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(afsFlatPooled2[0:5000, 3].mean())\n",
    "afsFlatPooled2[5000:, 3].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"empirical rr for both\", afsFlatPooled4[0:5000, 3].mean()/ afsFlatPooled4[5000:, 3].mean())\n",
    "print(\"empirical rr for 1\", ((afsFlatPooled4[0:2000, 1].mean()))/ afsFlatPooled4[2000:4000, 1].mean())\n",
    "print(\"empirical rr for 2\", ((afsFlatPooled4[2000:4000, 2].mean() + afsFlatPooled4[4000:5000, 2].mean())/2)/ afsFlatPooled4[:2000, 2].mean())\n",
    "# print(\"nullLikelihoodGlobal 1\", (nullLikelihoodsGlobal[0:2000, 0] + nullLikelihoodsGlobal[4000:5000, 0]).mean(), nullLikelihoodsGlobal[2000:, 0].mean())\n",
    "# print(\"nullLikelihoodGlobal 2\", nullLikelihoodsGlobal[2000:4000, 1].mean(), nullLikelihoodsGlobal[4000:, 1].mean())\n",
    "# print(\"nullLikelihoodGlobal Both\", nullLikelihoodsGlobal[4000:5000, 2].mean(), nullLikelihoodsGlobal[0:4000, 2].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llPooledBivariateSingleGene(tensor([10.,1.,1.,20000.]), tensor([.01,.01,.05]), tensor(13.), tensor(10.), tensor(10.), tensor(100000.), tensor(.77), tensor(.1), tensor(.1), tensor(.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this gives -2.401 log(likelihoodUnivariateSingleGene(xCtrl = 10, xCase1 = 1, prevalence1 = .01, pi0 = .9, pi1 = .1, pDiseaseGivenVariant = .001))\n",
    "#tensor(-2.5290): llUnivariateSingleGeneJensen(xCtrl = tensor(10.), xCase = tensor(1.), pD = tensor(.01), pi0 = tensor(.9), pi1 = tensor(.1), pDgivenV = tensor(.001))\n",
    "r = llUnivariateSingleGeneNoJensen(xCtrl = tensor(10.), xCase = tensor(1.), pD = tensor(.01), pi0 = tensor(.9), pi1 = tensor(.1), pDgivenV = tensor(.001))\n",
    "assert(abs(-r + tensor(-2.4010)) < .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altCounts = tensor([10., 2., 3., 1.])\n",
    "n = altCounts.sum()\n",
    "\n",
    "testAlpha = tensor([16., 20., 30., 15.])\n",
    "print(f\"test data: testAlpha: {testAlpha}, n: {n}, altCounts: {altCounts}\")\n",
    "DirichletMultinomial(total_count=n, concentration=testAlpha).log_prob(altCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDgivenV(.01, afsByGene[0:2000, 0, 1], 1e-4).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test functions\n",
    "pDgivenV(.01, afsByGeneRR2[0:2000, 0, 1], afsByGeneRR2Shape5[0:2000, 0, 0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance is wrong\n",
    "def betaVariance(alpha, beta):\n",
    "    return (alpha * beta) / ( ((alpha + beta)**2) + (alpha + beta + 1) )\n",
    "\n",
    "def betaMean(alpha, beta):\n",
    "    return alpha / (alpha + beta)\n",
    "\n",
    "print(\"variance\", betaVariance(6.47e1,5.39e3))\n",
    "print(\"mean\", betaMean(6.47e1,5.39e3))\n",
    "print(\"true varianc\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = afsByGene[0:2000, 0, 1].mean()\n",
    "m2 = afsByGeneRR2[0:2000, 0, 1].mean()/afsByGeneRR2[2000:, 0, 1].mean()\n",
    "m1 - m2\n",
    "print(m1, m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res = fitFnUniveriate(altCountsByGene, pDs, nEpochs=20, minLLThresholdCount=20, debug=True)\n",
    "print((time.time() - start) / 20, \"per iteration\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriate(altCountsByGeneRR2, pDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDgivenV(pDs[0], afsByGeneRR2Shape5[0:2000, 0, 1], afsByGeneRR2Shape5[0:2000, 0, 0]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDgivenV?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriate(altCounts, pDs, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriateBetaBinomial(altCountsByGeneRR2, pDs, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriateBetaBinomial(altCountsByGeneRR2Shape5, pDs, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(2.23307950e+02, 2.52700651e+04).sample([10_000,]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(2.20865706e+02, 1.73544747e+04).sample([10_000,]).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriateBetaBinomial(altCountsByGeneRR3, pDs, nEpochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(2.50432693e+02, 1.87756988e+04).sample([10_000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriate(altCountsByGeneRR3, pDs, nEpochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(3.84376856e+02, 2.37879954e+04).sample([10_000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsRR2Shape5 = []\n",
    "for i in range(1):\n",
    "    res = fitFnUniveriateBetaBinomial(altCountsByGeneRR2Shape5, pDs, nEpochs=50, minLLThresholdCount=50, debug=False)\n",
    "    resultsRR2Shape5.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsRR2Shape5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(1.96912591e+02, 1.61461738e+04).sample([10000,]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriateBetaBinomial(altCountsByGene, pDs, nEpochs=50, minLLThresholdCount=50, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(3.30289057e+03, 2.94460355e+04).sample([10000,]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't really work resConstrained = fitFnUniveriateBetaBinomialConstrained(altCountsByGeneRR2Shape5, pDs, nEpochs=10, minLLThresholdCount=10, debug=True)\n",
    "#resConstrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnUniveriateBetaBinomial(altCountsByGene, pDs, nEpochs=10, minLLThresholdCount=10, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"trueMeanPDVs\": [], \"truePis\": []}\n",
    "cachedData = [[altCountsByGenePooledCtrls, afsByGenePooledCtrls, {\n",
    "            \"nCases\": nCasesLarge,\n",
    "            \"nCtrls\": nCtrlsLarge,\n",
    "            \"pDs\": pDsGlobalLarge,\n",
    "            \"diseaseFractions\": diseaseFractions,\n",
    "            \"rrShape\": rrShape,\n",
    "            \"rrMeans\": rrMeans,\n",
    "            \"afShape\": afShape,\n",
    "            \"afMean\": afMean\n",
    "        }]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params2 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"trueMeanPDVs\": [], \"truePis\": [], \"costFnIdx\": []}\n",
    "cachedData2 = [[altCountsByGenePooledCtrls2, afsByGenePooledCtrls2, {\n",
    "            \"nCases\": nCasesLarge,\n",
    "            \"nCtrls\": nCtrlsLarge,\n",
    "            \"pDs\": pDsGlobalLarge,\n",
    "            \"diseaseFractions\": diseaseFractions,\n",
    "            \"rrShape\": rrShape,\n",
    "            \"rrMeans\": rrMeans,\n",
    "            \"afShape\": afShape,\n",
    "            \"afMean\": afMean,\n",
    "        }]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i >= len(cachedData):\n",
    "        start = time.time()\n",
    "        altCountsByGenePooledCtrls, afsByGenePooledCtrls = genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeans, afMean=afMean, afShape=afShape)\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData.append([altCountsByGenePooledCtrls, afsByGenePooledCtrls, {\n",
    "            \"nCases\": nCasesLarge,\n",
    "            \"nCtrls\": nCtrlsLarge,\n",
    "            \"pDs\": pDsGlobalLarge,\n",
    "            \"diseaseFractions\": diseaseFractions,\n",
    "            \"rrShape\": rrShape,\n",
    "            \"rrMeans\": rrMeans,\n",
    "            \"afShape\": afShape,\n",
    "            \"afMean\": afMean\n",
    "        }])\n",
    "        \n",
    "    res = fitFnBivariate(cachedData[i][0], pDsGlobalLarge, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=0)\n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3])\n",
    "    print(\"inferredPis\", inferredPis)\n",
    "    inferredAlphas = tensor(bestRes[3:])\n",
    "    print(\"inferredAlphas\", inferredAlphas)\n",
    "    \n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "\n",
    "    params[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params[\"inferredPis\"].append(inferredPis)\n",
    "    params[\"inferredPDVs\"].append(inferredPDs)\n",
    "\n",
    "    truth1 = pDgivenV(pDsGlobalLarge[0], afsByGenePooledCtrls[0:2000, 0, 1], afsByGenePooledCtrls[0:2000, 0, 0]).mean()\n",
    "    truth2 = pDgivenV(pDsGlobalLarge[1], afsByGenePooledCtrls[2000:4000, 1, 1], afsByGenePooledCtrls[2000:4000, 0, 0]).mean()\n",
    "    truth3 = pDgivenV(pDsGlobalLarge[2], afsByGenePooledCtrls[4000:5000, 2, 1], afsByGenePooledCtrls[4000:5000, 0, 0]).mean()\n",
    "    truth0 = 1 - (truth1 + truth2 + truth3)\n",
    "    print(\"truth0\", truth0, \"truth1\", truth1, \"truth2\", truth2, \"truthBoth\", truth3)\n",
    "\n",
    "    params[\"trueMeanPDVs\"].append(tensor([truth0, truth1, truth2, truth3]))\n",
    "    params[\"truePis\"].append(tensor(diseaseFractions))\n",
    "\n",
    "    print(f\"params on run {i}\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i >= len(cachedData2):\n",
    "        start = time.time()\n",
    "        altCountsByGenePooledCtrls2, afsByGenePooledCtrls2 = genDataSequentialPooledCtrls(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeans, afMean=afMean, afShape=afShape)\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData2.append([altCountsByGenePooledCtrls2, afsByGenePooledCtrls2, {\n",
    "            \"nCases\": nCasesLarge,\n",
    "            \"nCtrls\": nCtrlsLarge,\n",
    "            \"pDs\": pDsGlobalLarge,\n",
    "            \"diseaseFractions\": diseaseFractions,\n",
    "            \"rrShape\": rrShape,\n",
    "            \"rrMeans\": rrMeans,\n",
    "            \"afShape\": afShape,\n",
    "            \"afMean\": afMean,\n",
    "        }])\n",
    "    runCostFnIdx = 6\n",
    "    # todo append all entries to indciate failure\n",
    "    params2[\"costFnIdx\"].append(runCostFnIdx)\n",
    "\n",
    "    res = fitFnBivariate(cachedData2[i][0], pDsGlobalLarge, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3])\n",
    "    print(\"inferredPis\", inferredPis)\n",
    "    inferredAlphas = tensor(bestRes[3:])\n",
    "    print(\"inferredAlphas\", inferredAlphas)\n",
    "    \n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "\n",
    "    params2[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params2[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params2[\"inferredPis\"].append(inferredPis)\n",
    "    params2[\"inferredPDVs\"].append(inferredPDs)\n",
    "\n",
    "    truth1 = pDgivenV(pDsGlobalLarge[0], cachedData2[i][1][0:2000, 0, 1], cachedData2[i][1][0:2000, 0, 0]).mean()\n",
    "    truth2 = pDgivenV(pDsGlobalLarge[1], cachedData2[i][1][2000:4000, 1, 1], cachedData2[i][1][2000:4000, 0, 0]).mean()\n",
    "    truth3 = pDgivenV(pDsGlobalLarge[2], cachedData2[i][1][4000:5000, 2, 1], cachedData2[i][1][4000:5000, 0, 0]).mean()\n",
    "    truth0 = 1 - (truth1 + truth2 + truth3)\n",
    "    print(\"truth0\", truth0, \"truth1\", truth1, \"truth2\", truth2, \"truthBoth\", truth3)\n",
    "\n",
    "    params2[\"trueMeanPDVs\"].append(tensor([truth0, truth1, truth2, truth3]))\n",
    "    params2[\"truePis\"].append(tensor(diseaseFractions))\n",
    "    \n",
    "    print(f\"params on run {i}\", params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params3 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"trueMeanPDVs\": [], \"truePis\": [], \"costFnIdx\": []}\n",
    "cachedData3 = [[altCountsByGenePooledCtrls3, afsByGenePooledCtrls3, {\n",
    "            \"nCases\": nCasesLarge,\n",
    "            \"nCtrls\": nCtrlsLarge,\n",
    "            \"pDs\": pDsGlobalLarge,\n",
    "            \"diseaseFractions\": diseaseFractions,\n",
    "            \"rrShape\": rrShape,\n",
    "            \"rrMeans\": rrMeans,\n",
    "            \"afShape\": afShape,\n",
    "            \"afMean\": afMean,\n",
    "        }]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"truePis\": [], \"costFnIdx\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedData4 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 5):\n",
    "    if i >= len(cachedData4):\n",
    "        \n",
    "        params = genParams()[0]\n",
    "        start = time.time()\n",
    "        xsPooledRun, afsPooledRun, affectedGenesRun, unaffectedGenesRun = genData4(**params)\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData4.append({\n",
    "            \"xsPooled\": xsPooledRun,\n",
    "            \"afsPooled\": afsPooledRun,\n",
    "            \"affectedGenes\": affectedGenesRun,\n",
    "            \"unaffectedGenes\": unaffectedGenesRun,\n",
    "            \"params\": params,\n",
    "        })\n",
    "    cd = cachedData4[i]\n",
    "    xsPooledRun = cd[\"xsPooled\"]\n",
    "    afsPooledRun = cd[\"afsPooled\"]\n",
    "    affectedGenesRun = cd[\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cd[\"unaffectedGenes\"]\n",
    "\n",
    "    pDsRun = cd[\"params\"][\"pDs\"]\n",
    "    pisRun = cd[\"params\"][\"diseaseFractions\"]\n",
    "\n",
    "    runCostFnIdx = 15\n",
    "    res = fitFnBivariate(xsPooledRun, pDsRun, nEpochs=3, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / (inferredPis[0] + inferredPis[2])\n",
    "    inferredPiProp2 = inferredPis[1] / (inferredPis[1] + inferredPis[2])\n",
    "    \n",
    "    PDctrlV = inferredPDs[0]\n",
    "    PD1V = inferredPDs[1]\n",
    "    PD2V = inferredPDs[2]\n",
    "    PDBothV = inferredPDs[1]\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * PD1V + (1 - inferredPiProp1) * (PD1V + PDBothV * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * PD2V + (1 - inferredPiProp2) * (PD2V + PDBothV * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * PD1V * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * PD2V * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * PDBothV\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    \n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsPooledRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsPooledRun[affectedGenesRun[0], 0, 1].mean() + (1-piProp1) * afsPooledRun[affectedGenesRun[2], 0, 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsPooledRun[affectedGenesRun[1], 1, 1].mean() + (1-piProp2) * afsPooledRun[affectedGenesRun[2], 1, 1].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsPooledRun[affectedGenesRun[0], 2, 1].mean() + (pisRun[1] / pisRun.sum()) * afsPooledRun[affectedGenesRun[1], 2, 1].mean() + (pisRun[2] / pisRun.sum()) * afsPooledRun[affectedGenesRun[2], 2, 1].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    \n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs raw\", inferredPDs)\n",
    "    print(\"\\ninferredPDVs scaled\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params4[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params4[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params4[\"inferredPis\"].append(inferredPis)\n",
    "    params4[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params4[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params4[\"truePis\"].append(pisRun)\n",
    "    \n",
    "    params4[\"costFnIdx\"].append(runCostFnIdx)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "\n",
    "PV_hat = afsPooledRun[unaffectedGenesRun].mean()\n",
    "\n",
    "afs1 = afsPooledRun[affectedGenesRun[0], 0, 1].mean()\n",
    "piProp1 * afsPooledRun[affectedGenesRun[0], 0, 1].mean() + (1-piProp1) * ( afsPooledRun[affectedGenesRun[2], 0, 1].mean() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "inferredPiProp3 = inferredPis[2] / inferredPis.sum()\n",
    "inferredPis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredPiProp1 = inferredPis[0] / (inferredPis[0] + inferredPis[2])\n",
    "PDctrlV = inferredPDs[0]\n",
    "PD1V = inferredPDs[1]\n",
    "PD2V = inferredPDs[2]\n",
    "PDBothV = inferredPDs[1]\n",
    "\n",
    "inferredC1PDgivenV = inferredPiProp1 * PD1V + (1 - inferredPiProp1) * (PD1V + PDBothV * pDsRun[0]/pDsRun[2])\n",
    "inferredC2PDgivenV = inferredPiProp2 * PD2V + (1 - inferredPiProp2) * (PD2V + PDBothV * pDsRun[1]/pDsRun[2])\n",
    "inferredCBothPDgivenV = inferredPiProp1 * PD1V * pDsRun[2]/pDsRun[0] + inferredPiProp2 * PD2V * pDsRun[2]/pDsRun[1] + inferredPiProp3 * PDBothV\n",
    "\n",
    "inferredC1PDgivenV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piProp2 * afsPooledRun[affectedGenesRun[1], 1, 1].mean() + (1-piProp2) * afsPooledRun[affectedGenesRun[2], 1, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afsPooledRun[affectedGenesRun[2], 0, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "afsPooledRun[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "piProp1 * afsPooledRun[affectedGenesRun[0], 0, 1].mean() + (1-piProp1) * afsPooledRun[affectedGenesRun[2], 0, 1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "\n",
    "inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "\n",
    "inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "inferredCBothPDgivenV = inferredPiProp1 * inferredPDs[1] * pDsRun[2]/pDsRun[0] + inferredPiProp2 * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (1 - inferredPiProp1 - inferredPiProp2) * inferredPDs[3]\n",
    "inferredCBothPDgivenV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4b = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"truePis\": [], \"costFnIdx\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedData4b = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    if i >= len(cachedData4b):\n",
    "        # In DSB:\n",
    "        # \tNo ID\tID\t\n",
    "        #         ASD+ADHD\t684\t217\t\n",
    "        #         ASD\t3091\t871\t\n",
    "        #         ADHD\t3206\t271\t\n",
    "        #         Control\t5002\t-\t\n",
    "\n",
    "        #         gnomAD\t44779\t(Non-Finnish Europeans in non-psychiatric exome subset)\t\n",
    "\n",
    "        #         Case total:\t8340\t\t\n",
    "        #         Control total:\t49781\t\t\n",
    "        # so we can use pDBoth = .1 * total_cases\n",
    "        params = genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0]\n",
    "        start = time.time()\n",
    "        xsPooledRun, afsPooledRun, affectedGenesRun, unaffectedGenesRun = genData4(**params)\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData4b.append({\n",
    "            \"xsPooled\": xsPooledRun,\n",
    "            \"afsPooled\": afsPooledRun,\n",
    "            \"affectedGenes\": affectedGenesRun,\n",
    "            \"unaffectedGenes\": unaffectedGenesRun,\n",
    "            \"params\": params,\n",
    "        })\n",
    "    xsPooledRun = cachedData4b[i][\"xsPooled\"]\n",
    "    afsPooledRun = cachedData4b[i][\"afsPooled\"]\n",
    "    affectedGenesRun = cachedData4b[i][\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cachedData4b[i][\"unaffectedGenes\"]\n",
    "    pDsRun = cachedData4b[i][\"params\"][\"pDs\"]\n",
    "    pisRun = cachedData4b[i][\"params\"][\"diseaseFractions\"]\n",
    "\n",
    "    print(\"i is\", i)\n",
    "    print(\"params are:\", cachedData4b[i][\"params\"])\n",
    "    runCostFnIdx = 15\n",
    "    res = fitFnBivariate(xsPooledRun, pDsRun, nEpochs=10, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "    inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * inferredPDs[1] * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * inferredPDs[3]\n",
    "\n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsPooledRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsPooledRun[affectedGenesRun[0], 0, 1].mean() + (1-piProp1) * afsPooledRun[affectedGenesRun[2], 0, 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsPooledRun[affectedGenesRun[1], 1, 1].mean() + (1-piProp2) * afsPooledRun[affectedGenesRun[2], 1, 1].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsPooledRun[affectedGenesRun[0], 2, 1].mean() + (pisRun[1] / pisRun.sum()) * afsPooledRun[affectedGenesRun[1], 2, 1].mean() + (pisRun[2] / pisRun.sum()) * afsPooledRun[affectedGenesRun[2], 2, 1].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params4b[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params4b[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params4b[\"inferredPis\"].append(inferredPis)\n",
    "    params4b[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params4b[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params4b[\"truePis\"].append(pisRun)\n",
    "    \n",
    "    params4b[\"costFnIdx\"].append(runCostFnIdx)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params4b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4c = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"truePis\": [], \"costFnIdx\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "cachedData4c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pafor i in range(2):\n",
    "    if i >= len(cachedData4c):\n",
    "        # In DSB:\n",
    "        # \tNo ID\tID\t\n",
    "        #         ASD+ADHD\t684\t217\t\n",
    "        #         ASD\t3091\t871\t\n",
    "        #         ADHD\t3206\t271\t\n",
    "        #         Control\t5002\t-\t\n",
    "\n",
    "        #         gnomAD\t44779\t(Non-Finnish Europeans in non-psychiatric exome subset)\t\n",
    "\n",
    "        #         Case total:\t8340\t\t\n",
    "        #         Control total:\t49781\t\t\n",
    "        # so we can use pDBoth = .1 * total_cases\n",
    "        params = genParams(rrMeans=tensor([5., 5., 2.]), pis=tensor([.05, .05, .05]))[0]\n",
    "        start = time.time()\n",
    "        xsPooledRun, afsPooledRun, affectedGenesRun, unaffectedGenesRun = genData4c(**params)\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData4c.append({\n",
    "            \"xsPooled\": xsPooledRun,\n",
    "            \"afsPooled\": afsPooledRun,\n",
    "            \"affectedGenes\": affectedGenesRun,\n",
    "            \"unaffectedGenes\": unaffectedGenesRun,\n",
    "            \"params\": params,\n",
    "        })\n",
    "    xsPooledRun = cachedData4c[i][\"xsPooled\"]\n",
    "    afsPooledRun = cachedData4c[i][\"afsPooled\"]\n",
    "    affectedGenesRun = cachedData4c[i][\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cachedData4c[i][\"unaffectedGenes\"]\n",
    "    pDsRun = cachedData4c[i][\"params\"][\"pDs\"]\n",
    "    pisRun = cachedData4c[i][\"params\"][\"diseaseFractions\"]\n",
    "\n",
    "    print(\"i is\", i)\n",
    "    print(\"params are:\", cachedData4c[i][\"params\"])\n",
    "    runCostFnIdx = 15\n",
    "    res = fitFnBivariate(xsPooledRun, pDsRun, nEpochs=10, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)pa\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "    inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * inferredPDs[1] * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * inferredPDs[3]\n",
    "\n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsPooledRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsPooledRun[affectedGenesRun[0], 0, 1].mean() + (1-piProp1) * afsPooledRun[affectedGenesRun[2], 0, 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsPooledRun[affectedGenesRun[1], 1, 1].mean() + (1-piProp2) * afsPooledRun[affectedGenesRun[2], 1, 1].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsPooledRun[affectedGenesRun[0], 2, 1].mean() + (pisRun[1] / pisRun.sum()) * afsPooledRun[affectedGenesRun[1], 2, 1].mean() + (pisRun[2] / pisRun.sum()) * afsPooledRun[affectedGenesRun[2], 2, 1].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params4c[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params4c[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params4c[\"inferredPis\"].append(inferredPis)\n",
    "    params4c[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params4c[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params4c[\"truePis\"].append(pisRun)\n",
    "    \n",
    "    params4c[\"costFnIdx\"].append(runCostFnIdx)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params4c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params5 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"params\": [], \"costFnIdx\": [], \"generatingFn\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "\n",
    "cachedData5 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    if i >= len(cachedData5):\n",
    "        generatingFn = genData5\n",
    "        # In DSB:\n",
    "        # \tNo ID\tID\t\n",
    "        #         ASD+ADHD\t684\t217\t\n",
    "        #         ASD\t3091\t871\t\n",
    "        #         ADHD\t3206\t271\t\n",
    "        #         Control\t5002\t-\t\n",
    "\n",
    "        #         gnomAD\t44779\t(Non-Finnish Europeans in non-psychiatric exome subset)\t\n",
    "\n",
    "        #         Case total:\t8340\t\t\n",
    "        #         Control total:\t49781\t\t\n",
    "        # so we can use pDBoth = .1 * total_cases\n",
    "        params = genParams(rrMeans=tensor([5, 5, 2]), pis=tensor([.1, .1, .05]))\n",
    "        start = time.time()\n",
    "        r = generatingFn(**params[0])\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData5.append({**r, \"params\": params[0], \"generatingFn\": generatingFn})\n",
    "    print(\"params are:\", cachedData5[i][\"params\"])\n",
    "    xsRun = cachedData5[i][\"altCounts\"]\n",
    "    afsRun = cachedData5[i][\"afs\"]\n",
    "    affectedGenesRun = cachedData5[i][\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cachedData5[i][\"unaffectedGenes\"]\n",
    "    pDsRun = cachedData5[i][\"params\"][\"pDs\"]\n",
    "    pisRun = cachedData5[i][\"params\"][\"diseaseFractions\"]\n",
    "    paramsRun = cachedData5[i][\"params\"]\n",
    "\n",
    "    print(\"i is\", i)\n",
    "    print(\"pis are\", pisRun)\n",
    "    runCostFnIdx = 16\n",
    "    res = fitFnBivariate(xsRun, pDsRun, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "    inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * inferredPDs[1] * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * inferredPDs[3]\n",
    "\n",
    "    \n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsRun[affectedGenesRun[0], 1].mean() + (1-piProp1) * afsRun[affectedGenesRun[2], 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsRun[affectedGenesRun[1], 2].mean() + (1-piProp2) * afsRun[affectedGenesRun[2], 2].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsRun[affectedGenesRun[0], 3].mean() + (pisRun[1] / pisRun.sum()) * afsRun[affectedGenesRun[1], 3].mean() + (pisRun[2] / pisRun.sum()) * afsRun[affectedGenesRun[2], 3].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params5[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params5[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params5[\"inferredPis\"].append(inferredPis)\n",
    "    params5[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params5[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params5[\"params\"].append(paramsRun)\n",
    "    \n",
    "    params5[\"costFnIdx\"].append(runCostFnIdx)\n",
    "    params5[\"generatingFn\"].append(generatingFn)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params6 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"params\": [], \"costFnIdx\": [], \"generatingFn\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "\n",
    "cachedData6 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    if i >= len(cachedData6):\n",
    "        generatingFn = genData6\n",
    "        # In DSB:\n",
    "        # \tNo ID\tID\t\n",
    "        #         ASD+ADHD\t684\t217\t\n",
    "        #         ASD\t3091\t871\t\n",
    "        #         ADHD\t3206\t271\t\n",
    "        #         Control\t5002\t-\t\n",
    "\n",
    "        #         gnomAD\t44779\t(Non-Finnish Europeans in non-psychiatric exome subset)\t\n",
    "\n",
    "        #         Case total:\t8340\t\t\n",
    "        #         Control total:\t49781\t\t\n",
    "        # so we can use pDBoth = .1 * total_cases\n",
    "        params = genParams(rrMeans=tensor([3, 3, 1.5]), pis=tensor([.05, .05, .05]))\n",
    "        start = time.time()\n",
    "        r = generatingFn(**params[0])\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData6.append({**r, \"params\": params[0], \"generatingFn\": generatingFn})\n",
    "    print(\"params are:\", cachedData6[i][\"params\"])\n",
    "    xsRun = cachedData6[i][\"altCounts\"]\n",
    "    afsRun = cachedData6[i][\"afs\"]\n",
    "    affectedGenesRun = cachedData6[i][\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cachedData6[i][\"unaffectedGenes\"]\n",
    "    pDsRun = cachedData6[i][\"params\"][\"pDs\"]\n",
    "    pisRun = cachedData6[i][\"params\"][\"diseaseFractions\"]\n",
    "    paramsRun = cachedData6[i][\"params\"]\n",
    "\n",
    "    print(\"i is\", i)\n",
    "    print(\"pis are\", pisRun)\n",
    "    runCostFnIdx = 16\n",
    "    res = fitFnBivariate(xsRun, pDsRun, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "    inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * inferredPDs[1] * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * inferredPDs[3]\n",
    "\n",
    "    \n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsRun[affectedGenesRun[0], 1].mean() + (1-piProp1) * afsRun[affectedGenesRun[2], 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsRun[affectedGenesRun[1], 2].mean() + (1-piProp2) * afsRun[affectedGenesRun[2], 2].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsRun[affectedGenesRun[0], 3].mean() + (pisRun[1] / pisRun.sum()) * afsRun[affectedGenesRun[1], 3].mean() + (pisRun[2] / pisRun.sum()) * afsRun[affectedGenesRun[2], 3].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params6[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params6[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params6[\"inferredPis\"].append(inferredPis)\n",
    "    params6[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params6[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params6[\"params\"].append(paramsRun)\n",
    "    \n",
    "    params6[\"costFnIdx\"].append(runCostFnIdx)\n",
    "    params6[\"generatingFn\"].append(generatingFn)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params7 = {\"lls\": [], \"inferredAlphas\": [], \"inferredPis\": [], \"inferredPDVs\": [], \"truePDVs\": [], \"params\": [], \"costFnIdx\": []}\n",
    "# altCountsByGenePooledCtrls4, afsByGenePooledCtrls4, affectedGenes4 = genData4(nCases=nCasesLarge, nCtrls=nCtrlsLarge, pDs=pDsGlobalLarge, diseaseFractions=diseaseFractions, rrShape=rrShape, rrMeans=rrMeansCovary, afMean=afMean, afShape=afShape, nGenes=nGenes)\n",
    "\n",
    "cachedData7 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 50):\n",
    "    if i >= len(cachedData7):\n",
    "        # In DSB:\n",
    "        # \tNo ID\tID\t\n",
    "        #         ASD+ADHD\t684\t217\t\n",
    "        #         ASD\t3091\t871\t\n",
    "        #         ADHD\t3206\t271\t\n",
    "        #         Control\t5002\t-\t\n",
    "\n",
    "        #         gnomAD\t44779\t(Non-Finnish Europeans in non-psychiatric exome subset)\t\n",
    "\n",
    "        #         Case total:\t8340\t\t\n",
    "        #         Control total:\t49781\t\t\n",
    "        # so we can use pDBoth = .1 * total_cases\n",
    "        params = genParams(rrMeans=tensor([5., 5., 2]), pis=tensor([.1, .1, .05]))\n",
    "        start = time.time()\n",
    "        r = genData7(**params[0])\n",
    "\n",
    "        print(\"took\", time.time() - start)\n",
    "        cachedData7.append({**r, \"params\": params[0]})\n",
    "    cd = cachedData7[i]\n",
    "    print(f\"I is {i}\")\n",
    "    print(\"params are:\", cd[\"params\"], \"pis are\", pisRun)\n",
    "    xsRun = cd[\"altCounts\"]\n",
    "    afsRun = cd[\"afs\"]\n",
    "    affectedGenesRun = cd[\"affectedGenes\"]\n",
    "    unaffectedGenesRun = cd[\"unaffectedGenes\"]\n",
    "    pDsRun = cd[\"params\"][\"pDs\"]\n",
    "    pisRun = cd[\"params\"][\"diseaseFractions\"]\n",
    "\n",
    "    print(\"i is\", i)\n",
    "    print(\"pis are\", pisRun)\n",
    "    runCostFnIdx = 16\n",
    "    res = fitFnBivariate(xsRun, pDsRun, nEpochs=10, minLLThresholdCount=20, debug=True, costFnIdx=runCostFnIdx)\n",
    "    \n",
    "    bestRes = res[\"params\"][-1]\n",
    "\n",
    "    inferredPis = tensor(bestRes[0:3]) # 3-vector\n",
    "    inferredAlphas = tensor(bestRes[3:]) # 4-vector, idx0 is P(!D|V)\n",
    "\n",
    "    # index 0 is the P(!D|V), aka probability of being a control given variant present\n",
    "    inferredPDs = Dirichlet(concentration=inferredAlphas).sample([10_000,]).mean(0)\n",
    "    \n",
    "    inferredPiProp1 = inferredPis[0] / inferredPis.sum()\n",
    "    inferredPiProp2 = inferredPis[1] / inferredPis.sum()\n",
    "    \n",
    "    inferredC1PDgivenV = inferredPiProp1 * inferredPDs[1] + (1 - inferredPiProp1) * (inferredPDs[1] + inferredPDs[3] * pDsRun[0]/pDsRun[2])\n",
    "    inferredC2PDgivenV = inferredPiProp2 * inferredPDs[2] + (1 - inferredPiProp2) * (inferredPDs[2] + inferredPDs[3] * pDsRun[1]/pDsRun[2])\n",
    "    inferredCBothPDgivenV = (inferredPis[0] / inferredPis.sum()) * inferredPDs[1] * pDsRun[2]/pDsRun[0] + (inferredPis[1] / inferredPis.sum()) * inferredPDs[2] * pDsRun[2]/pDsRun[1] + (inferredPis[2] / inferredPis.sum()) * inferredPDs[3]\n",
    "    \n",
    "    piProp1 = pisRun[0] / (pisRun[0] + pisRun[2])\n",
    "    piProp2 = pisRun[1] / (pisRun[1] + pisRun[2])\n",
    "    \n",
    "    PV_hat = afsRun[unaffectedGenesRun].mean()\n",
    "    \n",
    "    trueC1PVgivenD = piProp1 * afsPooledRun[affectedGenesRun[0], 1].mean() + (1-piProp1) * afsPooledRun[affectedGenesRun[2], 1].mean()\n",
    "    trueC2PVgivenD = piProp2 * afsPooledRun[affectedGenesRun[1], 2].mean() + (1-piProp2) * afsPooledRun[affectedGenesRun[2], 2].mean()\n",
    "    trueCBothPVgivenD = (pisRun[0] / pisRun.sum()) * afsPooledRun[affectedGenesRun[0], 3].mean() + (pisRun[1] / pisRun.sum()) * afsPooledRun[affectedGenesRun[1], 3].mean() + (pisRun[2] / pisRun.sum()) * afsPooledRun[affectedGenesRun[2], 3].mean()\n",
    "    \n",
    "    trueC1PDgivenV = pDgivenV(pD=pDsRun[0], pVgivenD=trueC1PVgivenD, pV=PV_hat)\n",
    "    trueC2PDgivenV = pDgivenV(pD=pDsRun[1], pVgivenD=trueC2PVgivenD, pV=PV_hat)\n",
    "    trueCBothPDgivenV = pDgivenV(pD=pDsRun[2], pVgivenD=trueCBothPVgivenD, pV=PV_hat)\n",
    "    \n",
    "    inferredPDGivenVs = tensor([inferredC1PDgivenV, inferredC2PDgivenV, inferredCBothPDgivenV])\n",
    "    truePDGivenVs = tensor([trueC1PDgivenV, trueC2PDgivenV, trueCBothPDgivenV])\n",
    "    print(\"\\ninferredPis\", inferredPis)\n",
    "    print(\"\\ninferredPDVs\", inferredPDGivenVs)\n",
    "    print(\"\\ntruePDGivenVs\", truePDGivenVs)\n",
    "    \n",
    "    params7[\"lls\"].append(res[\"lls\"][-1])\n",
    "    params7[\"inferredAlphas\"].append(inferredAlphas)\n",
    "    params7[\"inferredPis\"].append(inferredPis)\n",
    "    params7[\"inferredPDVs\"].append(inferredPDGivenVs)\n",
    "    params7[\"truePDVs\"].append(truePDGivenVs)\n",
    "    params7[\"params\"].append(cd[\"params\"])\n",
    "    \n",
    "    params7[\"costFnIdx\"].append(runCostFnIdx)\n",
    "\n",
    "    print(f\"\\n\\nparams on run {i}\\n\", params7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50runs = {'lls': [58650.980808793975, 58663.01401779418, 58569.64432617293, 58783.93053623588, 58682.33104454364, 58670.7864442745, 58767.07466917903, 58782.95043266173, 58606.908820588906, 59304.74890821583, 58911.15021697714, 58777.76354377231, 58886.570525058676, 58979.60372626227, 58614.46769251791, 58575.018502609695, 58626.15989734368, 58342.02048699708, 58829.85280470915, 58523.63671041844, 59007.470544101496, 58665.29306683903, 58648.988256305376, 58598.11571418069, 58663.155895392454, 58532.66995278367, 58745.85232303206, 58644.916042740035, 58709.34026003936, 58809.63451601051, 58977.55303030012, 58892.08514286647, 58877.93190469771, 58620.8791816608, 58482.08335511814, 58771.290054508005, 58647.06512581756, 58771.13322666185, 59108.46648654986, 58352.932648069494, 58575.05496089697, 58791.550182731444, 58369.87373441178, 58822.57336510841, 58547.54381986277, 58687.756211119966, 58837.32699975922, 58466.909804709016, 58817.619849630566, 59005.57744894046, 58835.565050908466], 'inferredAlphas': [tensor([285.8449,  16.3664,  14.0267,   2.0217], dtype=torch.float64), tensor([ 741.8330, 4306.4346, 3780.8586, 2703.1299], dtype=torch.float64), tensor([ 827.7676, 4815.6097, 4234.2331, 2795.9735], dtype=torch.float64), tensor([ 262.6893, 1468.7320, 1280.2859,  960.5048], dtype=torch.float64), tensor([1070.0174, 6370.6853, 5331.2606, 3840.4440], dtype=torch.float64), tensor([1059.2849, 6294.1272, 5878.8562, 3665.3299], dtype=torch.float64), tensor([ 2058.0929, 12130.9712, 10666.2817,  6925.0974], dtype=torch.float64), tensor([ 244.4174, 1308.6004, 1285.9113,  996.4851], dtype=torch.float64), tensor([ 244.8058, 1382.5232, 1175.4547,  951.0711], dtype=torch.float64), tensor([ 253.0269, 1375.9456, 1155.1913,  971.0471], dtype=torch.float64), tensor([ 243.6885, 1279.8120, 1228.5845,  904.9808], dtype=torch.float64), tensor([ 227.2277, 1332.9335, 1045.9575,  837.2653], dtype=torch.float64), tensor([ 4491.7193, 25390.3203, 23260.1949, 16825.4658], dtype=torch.float64), tensor([ 210.3162, 1030.6505,  942.5924,  899.7928], dtype=torch.float64), tensor([ 951.5742, 5679.4705, 4692.2591, 3427.2280], dtype=torch.float64), tensor([ 2090.3471, 12572.5728, 11265.6956,  6863.4594], dtype=torch.float64), tensor([ 258.4622, 1358.8527, 1195.5652, 1029.1043], dtype=torch.float64), tensor([ 240.7183, 1422.7796, 1213.0808,  822.0767], dtype=torch.float64), tensor([ 218.6876, 1173.6052, 1008.0436,  913.3256], dtype=torch.float64), tensor([ 210.1855, 1198.3421,  950.7173,  774.5709], dtype=torch.float64), tensor([ 3885.8629, 22739.5771, 19526.8962, 14821.5338], dtype=torch.float64), tensor([ 327.3761, 1764.3421, 1621.4323, 1173.8440], dtype=torch.float64), tensor([ 347.1718, 1859.6708, 1685.6409, 1200.0646], dtype=torch.float64), tensor([ 299.7725, 1606.8455, 1396.6842, 1035.4529], dtype=torch.float64), tensor([ 257.2449, 1394.8654, 1207.4923, 1128.7950], dtype=torch.float64), tensor([ 2250.3099, 12833.0054, 12132.0743,  8453.1434], dtype=torch.float64), tensor([ 3716.1046, 22287.1198, 19425.6479, 12787.6191], dtype=torch.float64), tensor([1283.7207, 7219.4065, 6604.0128, 4735.5723], dtype=torch.float64), tensor([ 935.3248, 5416.2586, 4710.4235, 3779.6145], dtype=torch.float64), tensor([1033.9795, 5918.5535, 5207.8239, 3790.4872], dtype=torch.float64), tensor([ 321.1685, 1725.5269, 1467.2492, 1160.3267], dtype=torch.float64), tensor([ 286.3987, 1578.7462, 1465.8942, 1054.8159], dtype=torch.float64), tensor([ 5350.6208, 31668.0002, 29205.9696, 18821.1926], dtype=torch.float64), tensor([ 235.1930, 1262.0046, 1097.4198,  940.1415], dtype=torch.float64), tensor([ 259.1587, 1503.3844, 1281.0662,  846.0275], dtype=torch.float64), tensor([ 303.7221, 1760.4908, 1517.8713, 1049.1027], dtype=torch.float64), tensor([1076.3718, 6178.6256, 5626.6585, 3700.2391], dtype=torch.float64), tensor([ 254.6358, 1436.3133, 1255.2412,  939.2954], dtype=torch.float64), tensor([ 279.3273, 1441.6041, 1347.6220, 1101.8724], dtype=torch.float64), tensor([ 2272.5879, 13162.9147, 12073.3307,  8428.0081], dtype=torch.float64), tensor([ 305.1653, 1723.3509, 1518.7186, 1095.0070], dtype=torch.float64), tensor([ 226.5479, 1287.4487, 1058.2461,  626.2429], dtype=torch.float64), tensor([1503.1435, 8640.1354, 7890.9421, 4690.5015], dtype=torch.float64), tensor([ 242.4476, 1379.2745, 1170.9628,  842.5328], dtype=torch.float64), tensor([1696.1485, 9752.0705, 9307.7056, 5566.2307], dtype=torch.float64), tensor([ 4350.0141, 24539.3574, 22032.2995, 16896.9732], dtype=torch.float64), tensor([ 269.7695, 1583.3619, 1264.4152,  922.0513], dtype=torch.float64), tensor([ 235.7259, 1334.1986, 1223.2905,  735.6499], dtype=torch.float64), tensor([ 281.1943, 1628.7886, 1303.0426,  853.6718], dtype=torch.float64), tensor([ 2974.3975, 17608.0967, 15149.8580, 10894.3071], dtype=torch.float64), tensor([1663.6231, 9815.7876, 8915.7372, 5856.1519], dtype=torch.float64)], 'inferredPis': [tensor([0.0883, 0.0976, 0.0431], dtype=torch.float64), tensor([0.0888, 0.0960, 0.0436], dtype=torch.float64), tensor([0.0817, 0.0888, 0.0459], dtype=torch.float64), tensor([0.0864, 0.1012, 0.0461], dtype=torch.float64), tensor([0.0838, 0.0950, 0.0445], dtype=torch.float64), tensor([0.0848, 0.0805, 0.0429], dtype=torch.float64), tensor([0.0832, 0.0868, 0.0462], dtype=torch.float64), tensor([0.1021, 0.0883, 0.0335], dtype=torch.float64), tensor([0.0887, 0.0972, 0.0405], dtype=torch.float64), tensor([0.0907, 0.1154, 0.0468], dtype=torch.float64), tensor([0.1088, 0.0880, 0.0399], dtype=torch.float64), tensor([0.0661, 0.1068, 0.0480], dtype=torch.float64), tensor([0.0842, 0.0948, 0.0428], dtype=torch.float64), tensor([0.1099, 0.1108, 0.0452], dtype=torch.float64), tensor([0.0797, 0.0908, 0.0470], dtype=torch.float64), tensor([0.0871, 0.0829, 0.0440], dtype=torch.float64), tensor([0.0943, 0.1079, 0.0443], dtype=torch.float64), tensor([0.0714, 0.0921, 0.0434], dtype=torch.float64), tensor([0.0944, 0.1081, 0.0437], dtype=torch.float64), tensor([0.0857, 0.1092, 0.0421], dtype=torch.float64), tensor([0.0925, 0.1004, 0.0421], dtype=torch.float64), tensor([0.0965, 0.1002, 0.0434], dtype=torch.float64), tensor([0.0886, 0.0990, 0.0500], dtype=torch.float64), tensor([0.0849, 0.1075, 0.0473], dtype=torch.float64), tensor([0.1032, 0.1038, 0.0400], dtype=torch.float64), tensor([0.0894, 0.0908, 0.0398], dtype=torch.float64), tensor([0.0793, 0.0905, 0.0447], dtype=torch.float64), tensor([0.0841, 0.0890, 0.0463], dtype=torch.float64), tensor([0.0871, 0.0929, 0.0442], dtype=torch.float64), tensor([0.0857, 0.0859, 0.0455], dtype=torch.float64), tensor([0.0845, 0.1058, 0.0500], dtype=torch.float64), tensor([0.0891, 0.0930, 0.0436], dtype=torch.float64), tensor([0.0879, 0.0780, 0.0437], dtype=torch.float64), tensor([0.0943, 0.0986, 0.0446], dtype=torch.float64), tensor([0.0832, 0.0964, 0.0448], dtype=torch.float64), tensor([0.0853, 0.0872, 0.0450], dtype=torch.float64), tensor([0.0872, 0.0876, 0.0463], dtype=torch.float64), tensor([0.0913, 0.0833, 0.0422], dtype=torch.float64), tensor([0.1056, 0.0961, 0.0478], dtype=torch.float64), tensor([0.0817, 0.0893, 0.0398], dtype=torch.float64), tensor([0.0860, 0.0868, 0.0465], dtype=torch.float64), tensor([0.0682, 0.0968, 0.0603], dtype=torch.float64), tensor([0.0866, 0.0812, 0.0451], dtype=torch.float64), tensor([0.0847, 0.1019, 0.0477], dtype=torch.float64), tensor([0.0886, 0.0802, 0.0427], dtype=torch.float64), tensor([0.0909, 0.0928, 0.0422], dtype=torch.float64), tensor([0.0737, 0.1014, 0.0497], dtype=torch.float64), tensor([0.0824, 0.0799, 0.0473], dtype=torch.float64), tensor([0.0704, 0.1052, 0.0497], dtype=torch.float64), tensor([0.0821, 0.0994, 0.0440], dtype=torch.float64), tensor([0.0885, 0.0844, 0.0428], dtype=torch.float64)], 'inferredPDVs': [tensor([0.0708, 0.0619, 0.0089], dtype=torch.float64), tensor([1.0899, 1.0074, 0.1014], dtype=torch.float64), tensor([1.0667, 0.9848, 0.1029], dtype=torch.float64), tensor([1.1318, 1.0077, 0.1029], dtype=torch.float64), tensor([1.1056, 0.9851, 0.1022], dtype=torch.float64), tensor([1.0153, 1.0133, 0.1020], dtype=torch.float64), tensor([1.0521, 0.9876, 0.1029], dtype=torch.float64), tensor([1.0482, 1.1222, 0.0965], dtype=torch.float64), tensor([1.1388, 1.0359, 0.1011], dtype=torch.float64), tensor([1.1952, 1.0104, 0.1022], dtype=torch.float64), tensor([1.0185, 1.1137, 0.0989], dtype=torch.float64), tensor([1.2393, 0.9314, 0.1054], dtype=torch.float64), tensor([1.1085, 1.0210, 0.1023], dtype=torch.float64), tensor([1.1904, 1.1569, 0.1027], dtype=torch.float64), tensor([1.1210, 0.9949, 0.1050], dtype=torch.float64), tensor([1.0040, 0.9848, 0.1009], dtype=torch.float64), tensor([1.1806, 1.0640, 0.1024], dtype=torch.float64), tensor([1.1127, 0.9445, 0.1024], dtype=torch.float64), tensor([1.2040, 1.0772, 0.1028], dtype=torch.float64), tensor([1.1715, 0.9701, 0.0995], dtype=torch.float64), tensor([1.1101, 1.0163, 0.1003], dtype=torch.float64), tensor([1.0790, 1.0315, 0.1001], dtype=torch.float64), tensor([1.1042, 1.0182, 0.1044], dtype=torch.float64), tensor([1.1412, 0.9801, 0.1022], dtype=torch.float64), tensor([1.1735, 1.1224, 0.1005], dtype=torch.float64), tensor([1.0633, 1.0358, 0.1002], dtype=torch.float64), tensor([1.0750, 0.9687, 0.1022], dtype=torch.float64), tensor([1.0996, 1.0419, 0.1052], dtype=torch.float64), tensor([1.1437, 1.0632, 0.1049], dtype=torch.float64), tensor([1.0902, 1.0446, 0.1049], dtype=torch.float64), tensor([1.1739, 1.0088, 0.1053], dtype=torch.float64), tensor([1.0874, 1.0412, 0.1024], dtype=torch.float64), tensor([1.0148, 1.0381, 0.1029], dtype=torch.float64), tensor([1.1585, 1.0881, 0.1041], dtype=torch.float64), tensor([1.0711, 0.9495, 0.1004], dtype=torch.float64), tensor([1.0685, 1.0061, 0.1029], dtype=torch.float64), tensor([1.0485, 1.0132, 0.1030], dtype=torch.float64), tensor([1.0689, 1.0671, 0.1030], dtype=torch.float64), tensor([1.1077, 1.1350, 0.1048], dtype=torch.float64), tensor([1.0844, 1.0117, 0.1011], dtype=torch.float64), tensor([1.0880, 1.0398, 0.1050], dtype=torch.float64), tensor([1.0852, 0.8890, 0.1052], dtype=torch.float64), tensor([0.9926, 0.9855, 0.1011], dtype=torch.float64), tensor([1.1193, 0.9772, 0.1027], dtype=torch.float64), tensor([0.9850, 1.0100, 0.1006], dtype=torch.float64), tensor([1.1063, 1.0590, 0.1024], dtype=torch.float64), tensor([1.1593, 0.9396, 0.1044], dtype=torch.float64), tensor([1.0107, 0.9919, 0.1032], dtype=torch.float64), tensor([1.1219, 0.8802, 0.1013], dtype=torch.float64), tensor([1.1205, 0.9784, 0.1017], dtype=torch.float64), tensor([1.0317, 1.0187, 0.1015], dtype=torch.float64)], 'truePDVs': [tensor([6.3481e-04, 8.3878e-04, 4.1462e-05]), tensor([6.3481e-04, 8.3878e-04, 4.1462e-05]), tensor([6.3808e-04, 8.4310e-04, 4.1676e-05]), tensor([6.3467e-04, 8.3859e-04, 4.1453e-05]), tensor([6.3585e-04, 8.4015e-04, 4.1530e-05]), tensor([6.3759e-04, 8.4245e-04, 4.1644e-05]), tensor([6.3618e-04, 8.4059e-04, 4.1552e-05]), tensor([6.3485e-04, 8.3882e-04, 4.1464e-05]), tensor([6.3768e-04, 8.4256e-04, 4.1649e-05]), tensor([6.3740e-04, 8.4219e-04, 4.1631e-05]), tensor([6.3179e-04, 8.3479e-04, 4.1265e-05]), tensor([6.3569e-04, 8.3994e-04, 4.1520e-05]), tensor([6.3702e-04, 8.4170e-04, 4.1606e-05]), tensor([6.3582e-04, 8.4011e-04, 4.1528e-05]), tensor([6.3487e-04, 8.3886e-04, 4.1466e-05]), tensor([6.3620e-04, 8.4062e-04, 4.1553e-05]), tensor([6.3820e-04, 8.4325e-04, 4.1683e-05]), tensor([6.3967e-04, 8.4520e-04, 4.1779e-05]), tensor([6.3799e-04, 8.4298e-04, 4.1670e-05]), tensor([6.3464e-04, 8.3855e-04, 4.1451e-05]), tensor([6.3531e-04, 8.3944e-04, 4.1495e-05]), tensor([6.3777e-04, 8.4269e-04, 4.1655e-05]), tensor([6.3644e-04, 8.4093e-04, 4.1569e-05]), tensor([6.3583e-04, 8.4012e-04, 4.1529e-05]), tensor([6.3549e-04, 8.3967e-04, 4.1506e-05]), tensor([6.3602e-04, 8.4038e-04, 4.1541e-05]), tensor([6.3228e-04, 8.3544e-04, 4.1297e-05]), tensor([6.3543e-04, 8.3960e-04, 4.1503e-05]), tensor([6.3494e-04, 8.3895e-04, 4.1470e-05]), tensor([6.3507e-04, 8.3912e-04, 4.1479e-05]), tensor([6.3449e-04, 8.3835e-04, 4.1441e-05]), tensor([6.3523e-04, 8.3933e-04, 4.1490e-05]), tensor([6.3498e-04, 8.3900e-04, 4.1473e-05]), tensor([6.3650e-04, 8.4101e-04, 4.1572e-05]), tensor([6.3526e-04, 8.3937e-04, 4.1491e-05]), tensor([6.3655e-04, 8.4108e-04, 4.1576e-05]), tensor([6.3606e-04, 8.4043e-04, 4.1544e-05]), tensor([6.3609e-04, 8.4047e-04, 4.1546e-05]), tensor([6.3596e-04, 8.4030e-04, 4.1537e-05]), tensor([6.3583e-04, 8.4013e-04, 4.1529e-05]), tensor([6.3649e-04, 8.4100e-04, 4.1572e-05]), tensor([6.3450e-04, 8.3837e-04, 4.1442e-05]), tensor([6.3596e-04, 8.4030e-04, 4.1537e-05]), tensor([6.3761e-04, 8.4247e-04, 4.1645e-05]), tensor([6.3420e-04, 8.3797e-04, 4.1422e-05]), tensor([6.3418e-04, 8.3795e-04, 4.1421e-05]), tensor([6.3593e-04, 8.4026e-04, 4.1535e-05]), tensor([6.3837e-04, 8.4349e-04, 4.1695e-05]), tensor([6.3636e-04, 8.4082e-04, 4.1563e-05]), tensor([6.3534e-04, 8.3947e-04, 4.1497e-05]), tensor([6.3583e-04, 8.4013e-04, 4.1529e-05])], 'params': [{'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}, {'nGenes': 20000, 'nCases': tensor([5000., 5000., 1000.]), 'nCtrls': tensor(500000.), 'pDs': tensor([0.0098, 0.0098, 0.0020]), 'diseaseFractions': tensor([0.1000, 0.1000, 0.0500]), 'rrShape': tensor(10.), 'rrMeans': tensor([5., 5., 2.]), 'afShape': tensor(10.), 'afMean': tensor(1.0000e-04)}], 'costFnIdx': [15, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res50runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lastIdx = len(cachedData7) - 1\n",
    "cd = cachedData7[lastIdx]\n",
    "alphas = params7[\"inferredAlphas\"][lastIdx + 1]\n",
    "pds = tensor([1 - cd[\"params\"][\"pDs\"].sum(), *cd[\"params\"][\"pDs\"]])\n",
    "print('pds', pds)\n",
    "individualProbabilities = Dirichlet(alphas*pds).sample([10_000]).mean(0)\n",
    "print(\"alphas\", alphas)\n",
    "print(\"P(D|V)'s'\", individualProbabilities)\n",
    "pis = params7[\"inferredPis\"][lastIdx + 1]\n",
    "print(\"pis\", pis)\n",
    "params = params7[\"params\"][0]\n",
    "alphas\n",
    "print(params.__repr__())\n",
    "print(cd[\"params\"])\n",
    "\n",
    "inferredAlphas = tensor([x.numpy() for x in params7[\"inferredAlphas\"][1:]])\n",
    "avg = inferredAlphas.mean(0)\n",
    "std = inferredAlphas.std(0)\n",
    "minimum = inferredAlphas.min(0)\n",
    "maximum = inferredAlphas.max(0)\n",
    "\n",
    "print(\"avg\", avg, \"std\", std, \"min\", minimum, \"max\", maximum)\n",
    "\n",
    "inferredPDVs = tensor([Dirichlet(x*pds).sample([10_000]).mean(0).numpy() for x in params7[\"inferredAlphas\"][1:]])\n",
    "avg = inferredPDVs.mean(0)\n",
    "std = inferredPDVs.std(0)\n",
    "minimum = inferredPDVs.min(0)\n",
    "maximum = inferredPDVs.max(0)\n",
    "\n",
    "print(\"avg P(D|V)\", avg, \"std P(D|V)\", std, \"min P(D|V)\", minimum, \"max P(D|V)\", maximum)\n",
    "\n",
    "\n",
    "params7[\"params\"]\n",
    "\n",
    "params7[\"params\"][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latentAlphas = tensor([10., 10., 5.])\n",
    "oneA = tensor([1000., latentAlphas[0] +latentAlphas[2] , latentAlphas[1] +latentAlphas[2],latentAlphas.sum()])#inferredAlphas[0]\n",
    "oneAsum = oneA.sum()\n",
    "print(oneA, \"before\", inferredAlphas[0], \"sim\",oneAsum)\n",
    "CovX1X3 = (-oneA[1]*oneA[3] ) / ((oneAsum**2)*(oneAsum+1))\n",
    "CovX1X3\n",
    "Dirichlet(oneA).sample([10_00000]).mean(0)\n",
    "\n",
    "# P(V|D) = P(D|V)P(V) / P(D)\n",
    "# P(D|V) = rr * P(D|!V) * P(V)  / P(D)\n",
    "# = rr * P(D|!V) * P(V) / (P(D|V)PV + P(D|!V)P(!V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = cachedData7[1]\n",
    "np.corrcoef(cd[\"afs\"][0:1000, 0], cd[\"afs\"][0:1000, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd = cachedData4b[0][\"xsPooled\"]\n",
    "# print(cd[:, 0, 0][0])\n",
    "\n",
    "# cdCtrl = cd[:, 0, 0]\n",
    "# cdCases = cd[:, :, 1]\n",
    "\n",
    "# cdFlat = []\n",
    "# for geneIdx in range(20_000):\n",
    "#     cdFlat.append([cdCtrl[geneIdx], *cdCases[geneIdx].flatten()])\n",
    "# cdFlat = tensor(cdFlat)\n",
    "# cdFlat.type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdFlat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Multinomial(probs=tensor([.5, .5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.log_prob(tensor([5., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedData4b[-1][\"xsPooled\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCtrlsRun = tensor(5e5)\n",
    "nCases12 = tensor([5e3, 5e3])\n",
    "nCasesBoth = nCases12.sum() * .1\n",
    "nCasesRun = tensor([*nCases12, nCasesBoth])\n",
    "\n",
    "pDsRun = nCasesRun / ( nCasesRun.sum() + nCtrlsRun )\n",
    "pDsRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unaffectedGenesRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(cachedData4[0][0][4000:5000, 0, 0], cachedData4[0][0][4000:5000, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdAlts = cachedData4[0][0]\n",
    "cdAfs = cachedData4[0][1]\n",
    "cdGenes = cachedData4[0][2]\n",
    "cdPDs = cachedData4[0][3][\"pDs\"]\n",
    "cdPDs\n",
    "# pDgivenV(cdPDs[0], cdAfs[cdGenes[0], 0, 1], cdAfs[genesRun[0], 0, 0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdAlts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating P(D|V) for sample 1:\n",
    "# First, we know which genes are affected, the first pi1*nGenes, and then some genes that affect both diseases\n",
    "cdAfs[0:2000, 0, 1].mean()/cdAfs[0:2000, 0, 0].mean()\n",
    "s1PVgivenD = (2/3.0) * cdAfs[0:2000, 0, 1].mean() + (1/3.0) * cdAfs[4000:5000, 0, 1].mean()\n",
    "s1pD = cdPDs[0]\n",
    "s1PVhat = cdAfs[5000:, 0, 0].mean() # estimate allele frequency\n",
    "print(\"s1PVgivenD\", s1PVgivenD, \"s1pD\", s1pD, \"s1PVhat\", s1PVhat)\n",
    "# Note, we don't use 0:2000 for the estimate, because control allele frequency is P(V|!D)\n",
    "# and is depressed by the the presence of controls, e.g our P(V|!D) is proportional to P(V) - (P(V|D)*P(D)).sum()\n",
    "print(\"True P(D1|V)\", pDgivenV(pD=s1pD,pVgivenD=s1PVgivenD,pV=s1PVhat))\n",
    "\n",
    "# Now the estimated one\n",
    "pisInferred = params4[\"inferredPis\"][0]\n",
    "pi1inferred = pisInferred[0]\n",
    "piBothinferred = pisInferred[2]\n",
    "pi1ratio = (pi1inferred / (pi1inferred + piBothinferred))\n",
    "print(\"pi1inferred\", pi1inferred, \"piBothinferred\", piBothinferred, \"pi1ratio\", pi1ratio)\n",
    "pDsInferred = params4[\"inferredPDVs\"][0]\n",
    "pD1onlyInferred = pDsInferred[1]\n",
    "pDsharedInferred = pDsInferred[3]\n",
    "print(\"pD1onlyInferred inferred\", pD1onlyInferred, \"pDsharedInferred\", pDsharedInferred)\n",
    "print(\"Inferred P(D1|V)\", pD1onlyInferred * pi1ratio + (pD1onlyInferred + pDsharedInferred) * (1 - pi1ratio))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cdGenes[)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The P(D|V) for condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdAfs[cdGenes[0], 0, 1].mean() / cdAfs[cdGenes[0], 0, 0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params4, cachedData2 = runModel(altCountsByGenePooledCtrls2, afsByGenePooledCtrls2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = altCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = fitFnBivariate(cachedData[0][0], pDsLarge, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = fitFnBivariate(cachedData[0][0], pDsLarge, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res2 = fitFnBivariate(cachedData[0][0], pDsLarge, nEpochs=20, minLLThresholdCount=20, debug=True, costFnIdx=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"res0\", res0)\n",
    "print(\"\\nres0\", \"pis\", res0[\"params\"][-1][0:3], \"mean P(D|V)'s\", Dirichlet(tensor(res0[\"params\"][-1][3:])).sample([10_000]).mean(0))\n",
    "\n",
    "print(\"\\n\\n\\nres1\", res1)\n",
    "print(\"\\nres1\", \"pis\", res1[\"params\"][-1][0:3], \"mean P(D|V)'s\", Dirichlet(tensor(res1[\"params\"][-1][3:])).sample([10_000]).mean(0))\n",
    "\n",
    "print(\"\\n\\n\\nres2\", res2)\n",
    "print(\"\\nres2\", \"pis\", res2[\"params\"][-1][0:3], \"mean P(D|V)'s\", Dirichlet(tensor(res2[\"params\"][-1][3:])).sample([10_000]).mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(res0[\"llTrajectory\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(res1[\"llTrajectory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(res2[\"llTrajectory\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(afsByGenePooledCtrls[2000:4000, 1, 1]/afsByGenePooledCtrls[2000:4000, 0, 0]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pDgivenV(pD., afsByGenePooledCtrls[0:2000, :, 1], afsByGenePooledCtrls[0:2000, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Dirichlet(tensor(1/4.0).expand(4)).sample()\n",
    "test = test[0:3]\n",
    "r = [0,1,2,3]\n",
    "r[0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitFnBivariate(altCountsByGenePooledCtrls, pDs, nEpochs=100, minLLThresholdCount=100, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Dirichlet(concentration=tensor([1.40625703e+04,\n",
    "         5.56195520e+03, 1.57978682e+02, 2.33518936e+04]))\n",
    "d.sample([10_000,]).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(7.74788652e+02, 2.58170768e+04 + 9.72956833e+02 + 5.18278100e+03).sample([10000]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beta(3.05871723e+04, 3.25256694e+02 + 3.75135881e+03 +4.52942294e+04).sample([10000,]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "res = fitFnUniveriateBetaBinomial(altCountsByGene, pDs, nEpochs=100, minLLThresholdCount=100, debug=False)\n",
    "print(\"fitFnUniveriateBetaBinomial took for 100 epochs: \", time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.plot(res[\"llTrajectory\"])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomH0 = Binomial(total_count=tensor([1.,1]), probs=pDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomH0.log_prob(tensor(1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costFn2 = likelihoodUnivariateFast(altCountsByGene, pDs)\n",
    "# print(costFn2([1e-9, .999999]))\n",
    "print(costFn2([1e-9, 1e-9]))\n",
    "print(costFn2([0.08845797,0.11094360])) #gives ~12067 using jensen's method, and ~9887 using exponentiation of the log\n",
    "\n",
    "# best result from R\n",
    "#  0.08845797           0.11094360 , ll -10127.23, and with jensen's version, \"example -12037.4347455843\"\n",
    "# pDgivenV, pi1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costFn = likelihoodUnivariate(altCountsByGene, pDs)\n",
    "print(\"costFn1:\", costFn([.001, .01]),\"costFn2:\",costFn2([.001, .01]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costFn([0.0001,0.11094360])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(costFn2([0.0001,0.11094360]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Binomial(total_count=tensor([14., 0., 9.]), probs=tensor(.0099))\n",
    "d.log_prob(tensor([0.,0.,0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costFn2([1e-9, .999999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binomH0 = Binomial(total_count=geneSums, probs=.001)\n",
    "binomH1 = Binomial(total_count=geneSums, probs=.01)\n",
    "caseAltCounts = altCountsByGene[:, 0, 1]\n",
    "print(caseAltCounts)\n",
    "component0 = binomH0.log_prob(caseAltCounts)\n",
    "print(\"component0\", component0, .5*component0)\n",
    "component1 = binomH1.log_prob(caseAltCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pDgivenV(pDs[0], afsByGene2[0:2000, 0, 1].mean(), afMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = altCountsByGene2[:, 0, :]\n",
    "condition1\n",
    "pDs[0]\n",
    "\n",
    "afsByGene2[0:2000,:,1].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "pyplot.plot(afsByGene2[:, 0, 1:2].flatten())\n",
    "pyplot.plot(afsByGene2[:, 0, 0:1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot.figure(num=None, figsize=(20, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "pyplot.plot(cachedData6[0][\"altCounts\"][:, 3])\n",
    "# pyplot.plot(afsByGenePooledCtrls[:, 0, 1:2].flatten())\n",
    "# pyplot.plot(afsByGenePooledCtrls[:, 1, 1:2].flatten())\n",
    "# pyplot.plot(afsByGenePooledCtrls[:, 2, 1:2].flatten())\n",
    "# pyplot.plot(afsByGeneRR2[:, 0, 1:2].flatten())\n",
    "# pyplot.plot(afsByGeneRR2[:, 0, 0:1].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cachedData5[0][\"altCounts\"][0:5000].mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrlCounts = altCountsByGene[:, 0, 0]\n",
    "altCountsCases = altCountsByGene[:, :, 1]\n",
    "\n",
    "altCountsFlat = []\n",
    "for geneIdx in range(nGenes):\n",
    "    altCountsFlat.append([ctrlCounts[geneIdx], *altCountsByGene[geneIdx, :, 1].flatten()])\n",
    "altCountsFlat = tensor(altCountsFlat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altCountsFlat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DirichletMultinomial?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from torch.distributions import constraints\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "K = 4  # Fixed number of components.\n",
    "\n",
    "@config_enumerate\n",
    "def model(data):\n",
    "    # Global variables.\n",
    "    weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "    alpha0 = pyro.sample('alpha0', Uniform(1, 2.5e4))\n",
    "    alpha1 = pyro.sample('alpha0', Uniform(1, 2.5e4))\n",
    "    alpha2 = pyro.sample('alpha0', Uniform(1, 2.5e4))\n",
    "    alpha3 = pyro.sample('alpha0', Uniform(1, 2.5e4))\n",
    "\n",
    "    with pyro.plate('components', K):\n",
    "        concentrations = pyro.sample('concentrations', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "\n",
    "    with pyro.plate('data', len(data)):\n",
    "        # Local variables.\n",
    "        component = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        print(f\"concentrations: {concentrations[component]}\")\n",
    "        pyro.sample('obs', DirichletMultinomial(concentration=concentrations[component], total_count=data.sum(1)), obs=data)\n",
    "\n",
    "optim = pyro.optim.Adam({'lr': 0.1, 'betas': [0.8, 0.99]})\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loc_fn(site):\n",
    "    if site[\"name\"] == \"weights\":\n",
    "        # Initialize weights to uniform.\n",
    "        return torch.ones(K) / K\n",
    "    if site[\"name\"] == \"concentrations\":\n",
    "        return torch.ones(K) / K\n",
    "    raise ValueError(site[\"name\"])\n",
    "\n",
    "def initialize(seed):\n",
    "    global global_guide, svi\n",
    "    pyro.set_rng_seed(seed)\n",
    "    pyro.clear_param_store()\n",
    "    global_guide = AutoDelta(poutine.block(model, expose=['weights', 'concentrations']),\n",
    "                             init_loc_fn=init_loc_fn)\n",
    "    svi = SVI(model, global_guide, optim, loss=elbo)\n",
    "    return svi.loss(model, global_guide, altCountsFlat)\n",
    "\n",
    "# Choose the best among 100 random initializations.\n",
    "loss, seed = min((initialize(seed), seed) for seed in range(2))\n",
    "initialize(seed)\n",
    "print('seed = {}, initial_loss = {}'.format(seed, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks to monitor gradient norms.\n",
    "gradient_norms = defaultdict(list)\n",
    "for name, value in pyro.get_param_store().named_parameters():\n",
    "    value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item()))\n",
    "\n",
    "losses = []\n",
    "for i in range(200 if not smoke_test else 2):\n",
    "    loss = svi.step(altCountsFlat)\n",
    "    losses.append(loss)\n",
    "    print('.' if i % 100 else '\\n', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(10,3), dpi=100).set_facecolor('white')\n",
    "pyplot.plot(losses)\n",
    "pyplot.xlabel('iters')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.yscale('log')\n",
    "pyplot.title('Convergence of SVI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, grad_norms in gradient_norms.items():\n",
    "    pyplot.plot(grad_norms, label=name)\n",
    "pyplot.xlabel('iters')\n",
    "pyplot.ylabel('gradient norm')\n",
    "pyplot.yscale('log')\n",
    "pyplot.legend(loc='best')\n",
    "pyplot.title('Gradient norms during SVI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimates = global_guide(altCountsFlat)\n",
    "weights = map_estimates['weights']\n",
    "locs = map_estimates['concentrations']\n",
    "print('weights = {}'.format(weights.data.numpy()))\n",
    "print('concentrations = {}'.format(locs.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirichlet(tensor([0.8973397  , 0.0494441,  0.04917945, 0.00403667])).sample([10_000,]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "from torch.distributions import constraints\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro import poutine\n",
    "from pyro.infer.autoguide import AutoDelta\n",
    "from pyro.optim import Adam\n",
    "from pyro.infer import SVI, TraceEnum_ELBO, config_enumerate, infer_discrete\n",
    "\n",
    "smoke_test = ('CI' in os.environ)\n",
    "assert pyro.__version__.startswith('1.3.1')\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 2  # Fixed number of components.\n",
    "\n",
    "@config_enumerate\n",
    "def model(data):\n",
    "    # Global variables.\n",
    "    weights = pyro.sample('weights', dist.Dirichlet(0.5 * torch.ones(K)))\n",
    "    scale = pyro.sample('scale', dist.LogNormal(0., 2.))\n",
    "    with pyro.plate('components', K):\n",
    "        locs = pyro.sample('locs', dist.Normal(0., 10.))\n",
    "\n",
    "    with pyro.plate('data', len(data)):\n",
    "        # Local variables.\n",
    "        assignment = pyro.sample('assignment', dist.Categorical(weights))\n",
    "        pyro.sample('obs', dist.Normal(locs[assignment], scale), obs=data)\n",
    "        \n",
    "optim = pyro.optim.Adam({'lr': 0.1, 'betas': [0.8, 0.99]})\n",
    "elbo = TraceEnum_ELBO(max_plate_nesting=1)\n",
    "\n",
    "\n",
    "def init_loc_fn(site):\n",
    "    if site[\"name\"] == \"weights\":\n",
    "        # Initialize weights to uniform.\n",
    "        return torch.ones(K) / K\n",
    "    if site[\"name\"] == \"scale\":\n",
    "        return (data.var() / 2).sqrt()\n",
    "    if site[\"name\"] == \"locs\":\n",
    "        return data[torch.multinomial(torch.ones(len(data)) / len(data), K)]\n",
    "    raise ValueError(site[\"name\"])\n",
    "\n",
    "def initialize(seed):\n",
    "    global global_guide, svi\n",
    "    pyro.set_rng_seed(seed)\n",
    "    pyro.clear_param_store()\n",
    "    global_guide = AutoDelta(poutine.block(model, expose=['weights', 'locs', 'scale']),\n",
    "                             init_loc_fn=init_loc_fn)\n",
    "    svi = SVI(model, global_guide, optim, loss=elbo)\n",
    "    return svi.loss(model, global_guide, data)\n",
    "\n",
    "# Choose the best among 100 random initializations.\n",
    "loss, seed = min((initialize(seed), seed) for seed in range(100))\n",
    "initialize(seed)\n",
    "print('seed = {}, initial_loss = {}'.format(seed, loss))\n",
    "\n",
    "# Register hooks to monitor gradient norms.\n",
    "gradient_norms = defaultdict(list)\n",
    "for name, value in pyro.get_param_store().named_parameters():\n",
    "    value.register_hook(lambda g, name=name: gradient_norms[name].append(g.norm().item()))\n",
    "\n",
    "losses = []\n",
    "for i in range(200 if not smoke_test else 2):\n",
    "    loss = svi.step(data)\n",
    "    losses.append(loss)\n",
    "    print('.' if i % 100 else '\\n', end='')\n",
    "    \n",
    "\n",
    "pyplot.figure(figsize=(10,3), dpi=100).set_facecolor('white')\n",
    "pyplot.plot(losses)\n",
    "pyplot.xlabel('iters')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.yscale('log')\n",
    "pyplot.title('Convergence of SVI');\n",
    "\n",
    "\n",
    "pyplot.figure(figsize=(10,4), dpi=100).set_facecolor('white')\n",
    "for name, grad_norms in gradient_norms.items():\n",
    "    pyplot.plot(grad_norms, label=name)\n",
    "pyplot.xlabel('iters')\n",
    "pyplot.ylabel('gradient norm')\n",
    "pyplot.yscale('log')\n",
    "pyplot.legend(loc='best')\n",
    "pyplot.title('Gradient norms during SVI');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_estimates = global_guide(data)\n",
    "weights = map_estimates['weights']\n",
    "locs = map_estimates['locs']\n",
    "scale = map_estimates['scale']\n",
    "print('weights = {}'.format(weights.data.numpy()))\n",
    "print('locs = {}'.format(locs.data.numpy()))\n",
    "print('scale = {}'.format(scale.data.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import constraints\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "from pyro.ops.indexing import Vindex\n",
    "import pyro.distributions as pdist\n",
    "\n",
    "@config_enumerate\n",
    "def model(obs, n):\n",
    "\n",
    "    p_A = pyro.sample('p_A', pdist.D(1, 1))\n",
    "    \n",
    "    p_B = pyro.sample('p_B', dist.Beta(torch.ones(2), torch.ones(2)).to_event(1))\n",
    "    \n",
    "    p_C = pyro.sample('p_C', dist.Beta(torch.ones(2), torch.ones(2)).to_event(1))\n",
    "    \n",
    "    with pyro.plate('data_plate', n):\n",
    "        A = pyro.sample('A', dist.Bernoulli(p_A.expand(n)), obs=obs['A'])\n",
    "    \n",
    "        B = pyro.sample('B', dist.Bernoulli(Vindex(p_B)[A.type(torch.long)]), infer={\"enumerate\": \"parallel\"})\n",
    "        \n",
    "        pyro.sample('C', dist.Bernoulli(Vindex(p_C)[B.type(torch.long)]), obs=obs['C'])\n",
    "\n",
    "def guide(obs, n):\n",
    "    \n",
    "    a = pyro.param('a', prior['A'], constraint=constraints.positive)\n",
    "    p_A = pyro.sample('p_A', dist.Beta(a[0], a[1]))\n",
    "    \n",
    "    b = pyro.param('b', prior['B'], constraint=constraints.positive)\n",
    "    pyro.sample('p_B', dist.Beta(b[:, 0], b[:, 1]).to_event(1))\n",
    "    \n",
    "    c = pyro.param('c', prior['C'], constraint=constraints.positive)\n",
    "    pyro.sample('p_C', dist.Beta(c[:, 0], c[:, 1]).to_event(1))\n",
    "\n",
    "import pyro.optim\n",
    "import pyro.infer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pyro.enable_validation(True)\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# setup svi object\n",
    "loss_func = pyro.infer.TraceEnum_ELBO(max_plate_nesting=1)\n",
    "optim = pyro.optim.Adam({\"lr\": .001})\n",
    "svi = pyro.infer.SVI(model, guide, optim, loss=loss_func)\n",
    "\n",
    "# perform svi\n",
    "num_steps = 30000\n",
    "losses = []\n",
    "start = time.time()\n",
    "for step in range(num_steps):\n",
    "    loss = svi.step(data, n)\n",
    "    losses.append(loss)\n",
    "\n",
    "    if step % (num_steps*.33) == 0:\n",
    "        print(step, f'({(time.time() - start)/60:.1f} min.)')\n",
    "print(step+1, f'({(time.time() - start)/60:.1f} min.)\\n\\n')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.show()\n",
    "\n",
    "posterior_params = {k: np.array(v.data) for k, v in pyro.get_param_store().items()}\n",
    "posterior_params['a'] = posterior_params['a'][None, :]\n",
    "for key, val in posterior_params.items():\n",
    "    true_p = p[f'p_{key.upper()}'].numpy()\n",
    "    print(f'p_{key.upper()}  (true/pred): ')\n",
    "    print('\\t', np.round(true_p, 2))\n",
    "    print('\\t', np.round(val[:, 0]/(np.sum(val, axis=1)), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
